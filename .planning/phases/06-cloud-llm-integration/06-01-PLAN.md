---
phase: 06-cloud-llm-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [mcp-server/src/opta_mcp/claude.py, mcp-server/src/opta_mcp/server.py, mcp-server/pyproject.toml, src-tauri/src/claude.rs, src-tauri/src/lib.rs, src/types/claude.ts, .env.example]
autonomous: true
---

<objective>
Integrate Claude API for complex reasoning queries.

Purpose: Enable high-quality AI responses for complex optimization questions that benefit from Claude's reasoning.
Output: Python Claude client, Tauri commands, TypeScript types, API key configuration.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Prior phase (local LLM):
@.planning/phases/05-local-llm-integration/05-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Claude API client module</name>
  <files>mcp-server/src/opta_mcp/claude.py, mcp-server/pyproject.toml, .env.example</files>
  <action>
1. Add `anthropic` to pyproject.toml dependencies

2. Create `claude.py` module:
   ```python
   import os
   from anthropic import Anthropic

   def get_client():
       api_key = os.environ.get("ANTHROPIC_API_KEY")
       if not api_key:
           return None
       return Anthropic(api_key=api_key)

   def check_claude_status():
       client = get_client()
       if not client:
           return {"available": False, "error": "API key not configured"}
       try:
           # Simple test - list models or make minimal request
           return {"available": True, "model": "claude-sonnet-4-20250514"}
       except Exception as e:
           return {"available": False, "error": str(e)}

   def chat_completion(messages, system_prompt=None):
       client = get_client()
       if not client:
           return {"error": "API key not configured"}

       kwargs = {
           "model": "claude-sonnet-4-20250514",
           "max_tokens": 1024,
           "messages": messages
       }
       if system_prompt:
           kwargs["system"] = system_prompt

       response = client.messages.create(**kwargs)
       return {
           "content": response.content[0].text,
           "model": response.model,
           "usage": {
               "input_tokens": response.usage.input_tokens,
               "output_tokens": response.usage.output_tokens
           }
       }
   ```

3. Create `.env.example`:
   ```
   # Claude API Key (get from console.anthropic.com)
   ANTHROPIC_API_KEY=your-api-key-here
   ```

4. Register MCP tools:
   - `claude_status` - check if API configured
   - `claude_chat` - send chat to Claude

IMPORTANT: Never log or expose the API key. Handle missing key gracefully.
  </action>
  <verify>
cd mcp-server && uv run python -c "from opta_mcp.claude import check_claude_status; import json; print(json.dumps(check_claude_status(), indent=2))"
  </verify>
  <done>Claude API client created with status check and chat</done>
</task>

<task type="auto">
  <name>Task 2: Create Rust commands for Claude API</name>
  <files>src-tauri/src/claude.rs, src-tauri/src/lib.rs, src/types/claude.ts</files>
  <action>
1. Create `claude.rs` module:
   - `ClaudeStatus` struct: available (bool), model (Option<String>), error (Option<String>)
   - `ClaudeUsage` struct: input_tokens (u32), output_tokens (u32)
   - `ClaudeResponse` struct: content (String), model (String), usage (ClaudeUsage)
   - `#[tauri::command] async fn claude_status()` - check API availability
   - `#[tauri::command] async fn claude_chat(messages: Vec<ChatMessage>)` - send to Claude

2. Register in `lib.rs`

3. Create `src/types/claude.ts`:
   ```typescript
   export interface ClaudeStatus {
     available: boolean;
     model?: string;
     error?: string;
   }

   export interface ClaudeUsage {
     input_tokens: number;
     output_tokens: number;
   }

   export interface ClaudeResponse {
     content: string;
     model: string;
     usage: ClaudeUsage;
   }
   ```

Follow established patterns from llm.rs.
  </action>
  <verify>
cd src-tauri && cargo check
  </verify>
  <done>Rust commands for Claude API compile</done>
</task>

<task type="auto">
  <name>Task 3: Add Claude configuration to Settings</name>
  <files>src/pages/Settings.tsx, src/hooks/useClaude.ts</files>
  <action>
1. Create `useClaude.ts` hook:
   - Check status on mount
   - Return { status, loading, error, checkStatus }
   - Don't poll - check on-demand

2. Update Settings.tsx:
   - Add "Cloud AI" section
   - Show Claude status (configured/not configured)
   - Link to Anthropic console for API key
   - Show usage stats if available (tokens used this session)
   - Note: API key entered via environment variable, not UI (security)

3. Status display:
   - Green checkmark if API key configured
   - Warning if not configured with setup instructions
   - Show model being used (Claude Sonnet)

Keep it informational - API key management is outside the app.
  </action>
  <verify>npm run build</verify>
  <done>Settings shows Claude API status</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `cd mcp-server && uv pip install anthropic` succeeds
- [ ] Claude module imports without error
- [ ] `cd src-tauri && cargo check` passes
- [ ] `npm run build` succeeds
- [ ] Settings shows Claude status section
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- Claude API works when key is configured
- Graceful handling when key is missing
</success_criteria>

<output>
After completion, create `.planning/phases/06-cloud-llm-integration/06-01-SUMMARY.md`
</output>
