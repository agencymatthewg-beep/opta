---
phase: 06-cloud-llm-integration
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified: [mcp-server/src/opta_mcp/router.py, mcp-server/src/opta_mcp/server.py, src/hooks/useLlm.ts, src/components/ChatInterface.tsx]
autonomous: true
---

<objective>
Implement hybrid routing logic to choose between local Ollama and cloud Claude.

Purpose: Use free local LLM for simple queries, Claude for complex reasoning - optimizing cost and quality.
Output: Router module that classifies queries and routes to appropriate backend.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Prior plan (Claude API):
@.planning/phases/06-cloud-llm-integration/06-01-PLAN.md

Local LLM context:
@.planning/phases/05-local-llm-integration/05-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create query router module</name>
  <files>mcp-server/src/opta_mcp/router.py, mcp-server/src/opta_mcp/server.py</files>
  <action>
Create `router.py` module:

1. `classify_query(query: str) -> str` function:
   Returns "local" or "cloud" based on complexity analysis:

   ```python
   CLOUD_INDICATORS = [
       # Complex reasoning
       "why", "explain", "analyze", "compare", "recommend",
       # Multi-step
       "step by step", "walkthrough", "guide me",
       # Technical depth
       "architecture", "design", "optimize for",
       # Troubleshooting
       "debug", "diagnose", "not working", "error",
       # Long queries (>100 words likely need better reasoning)
   ]

   LOCAL_INDICATORS = [
       # Simple facts
       "what is", "how to", "list", "show",
       # Quick answers
       "quick", "simple", "basic",
       # Status checks
       "status", "check", "current"
   ]

   def classify_query(query: str) -> str:
       query_lower = query.lower()
       word_count = len(query.split())

       # Long queries -> cloud
       if word_count > 50:
           return "cloud"

       cloud_score = sum(1 for ind in CLOUD_INDICATORS if ind in query_lower)
       local_score = sum(1 for ind in LOCAL_INDICATORS if ind in query_lower)

       # Prefer local unless strong cloud signal
       if cloud_score > local_score + 1:
           return "cloud"
       return "local"
   ```

2. `route_chat(messages: list, prefer: str = "auto") -> dict`:
   - If prefer="local": always use Ollama
   - If prefer="cloud": always use Claude (if available)
   - If prefer="auto": use classifier
   - Returns response with source indicator

3. `get_routing_stats() -> dict`:
   - Track queries routed to each backend
   - Useful for cost monitoring

4. Register `smart_chat` MCP tool that uses router
  </action>
  <verify>
cd mcp-server && uv run python -c "from opta_mcp.router import classify_query; print(classify_query('How do I boost FPS?'), classify_query('Explain why my GPU is throttling and recommend solutions'))"
  </verify>
  <done>Router classifies queries and routes to appropriate backend</done>
</task>

<task type="auto">
  <name>Task 2: Update useLlm hook for hybrid routing</name>
  <files>src/hooks/useLlm.ts</files>
  <action>
Update `useLlm.ts` hook:

1. Add routing preference:
   ```typescript
   type RoutingPreference = "auto" | "local" | "cloud";

   interface UseLlmOptions {
     routingPreference?: RoutingPreference;
   }
   ```

2. Update sendMessage to use smart_chat:
   - Pass routing preference
   - Return which backend was used in response
   - Track usage stats

3. Add backend indicator to response:
   ```typescript
   interface ChatResult {
     content: string;
     backend: "local" | "cloud";
     model: string;
   }
   ```

4. Add setRoutingPreference to allow runtime changes

User should know which AI answered their question.
  </action>
  <verify>npm run build</verify>
  <done>useLlm hook supports hybrid routing</done>
</task>

<task type="auto">
  <name>Task 3: Update ChatInterface to show backend source</name>
  <files>src/components/ChatInterface.tsx, src/components/ChatMessage.tsx</files>
  <action>
1. Update ChatMessage.tsx:
   - Add optional `backend` prop
   - Show small badge on assistant messages: "Local" or "Claude"
   - Badge styling: local=muted, cloud=primary

2. Update ChatInterface.tsx:
   - Add routing preference toggle in header
   - Options: Auto (default), Local Only, Claude Only
   - Dropdown or segmented control
   - Show cost warning if switching to "Claude Only"

3. Message metadata:
   - Store backend used with each message
   - Display badge on hover or always (keep it subtle)

4. Header additions:
   - "AI Mode: Auto" indicator
   - Click to change mode

Keep UI clean - don't overwhelm, but give users control.
  </action>
  <verify>npm run dev (chat shows backend badges and mode toggle)</verify>
  <done>ChatInterface shows which AI backend responded</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Router classifies queries correctly
- [ ] `npm run build` succeeds
- [ ] Chat shows backend badges on messages
- [ ] Routing preference toggle works
- [ ] Auto mode routes simple queries locally
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- Hybrid routing optimizes cost while maintaining quality
- Users can override routing preference
</success_criteria>

<output>
After completion, create `.planning/phases/06-cloud-llm-integration/06-02-SUMMARY.md`
</output>
