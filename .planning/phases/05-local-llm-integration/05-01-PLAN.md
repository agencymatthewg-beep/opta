---
phase: 05-local-llm-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [mcp-server/src/opta_mcp/llm.py, mcp-server/src/opta_mcp/server.py, mcp-server/pyproject.toml, src-tauri/src/llm.rs, src-tauri/src/lib.rs, src/types/llm.ts]
autonomous: true
---

<objective>
Set up Ollama integration for local LLM inference with Llama 3 8B.

Purpose: Enable zero-cost AI queries for routine optimization questions using locally-running Llama 3.
Output: Python Ollama client, Tauri commands for LLM operations, TypeScript types.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Ollama client module in Python MCP server</name>
  <files>mcp-server/src/opta_mcp/llm.py, mcp-server/pyproject.toml</files>
  <action>
1. Add `ollama` to pyproject.toml dependencies

2. Create `llm.py` module with:
   - `check_ollama_status()` - verify Ollama is running, return available models
   - `pull_model(model_name: str)` - download model if not present
   - `chat_completion(messages: list, model: str = "llama3:8b")` - non-streaming chat
   - `get_available_models()` - list installed models

3. Ollama API integration:
   ```python
   import ollama

   def check_ollama_status():
       try:
           models = ollama.list()
           return {"running": True, "models": [m["name"] for m in models["models"]]}
       except Exception as e:
           return {"running": False, "error": str(e)}

   def chat_completion(messages, model="llama3:8b"):
       response = ollama.chat(model=model, messages=messages)
       return {
           "content": response["message"]["content"],
           "model": model,
           "done": True
       }
   ```

4. Register MCP tools in server.py:
   - `llm_status` - check if Ollama running
   - `llm_chat` - send chat message
   - `llm_models` - list available models

IMPORTANT: Handle Ollama not running gracefully - return clear error, don't crash.
  </action>
  <verify>
cd mcp-server && uv run python -c "from opta_mcp.llm import check_ollama_status; import json; print(json.dumps(check_ollama_status(), indent=2))"
  </verify>
  <done>Ollama client module created with status check and chat completion</done>
</task>

<task type="auto">
  <name>Task 2: Create Rust commands for LLM operations</name>
  <files>src-tauri/src/llm.rs, src-tauri/src/lib.rs, src/types/llm.ts</files>
  <action>
1. Create `llm.rs` module with:
   - `LlmStatus` struct: running (bool), models (Vec<String>), error (Option<String>)
   - `ChatMessage` struct: role (String), content (String)
   - `ChatResponse` struct: content (String), model (String), done (bool)
   - `#[tauri::command] async fn llm_status()` - check Ollama status
   - `#[tauri::command] async fn llm_chat(messages: Vec<ChatMessage>)` - send chat

2. Register in `lib.rs`:
   - Add `mod llm;`
   - Add commands to invoke_handler

3. Create `src/types/llm.ts`:
   ```typescript
   export interface LlmStatus {
     running: boolean;
     models: string[];
     error?: string;
   }

   export interface ChatMessage {
     role: "user" | "assistant" | "system";
     content: string;
   }

   export interface ChatResponse {
     content: string;
     model: string;
     done: boolean;
   }
   ```

Follow subprocess pattern from existing commands.
  </action>
  <verify>
cd src-tauri && cargo check
  </verify>
  <done>Rust commands compile for LLM status and chat</done>
</task>

<task type="auto">
  <name>Task 3: Create useLlm hook for frontend</name>
  <files>src/hooks/useLlm.ts</files>
  <action>
1. Create `useLlm.ts` hook:
   ```typescript
   interface UseLlmResult {
     status: LlmStatus | null;
     loading: boolean;
     error: string | null;
     sendMessage: (message: string) => Promise<string>;
     checkStatus: () => Promise<void>;
   }
   ```

2. Implementation:
   - Check status on mount
   - `sendMessage` builds messages array and calls llm_chat
   - Track conversation history internally
   - Handle errors gracefully (Ollama not running)

3. Status check:
   - Don't poll continuously (expensive)
   - Check on mount and on-demand via checkStatus()

Keep it simple - streaming will be added in Plan 05-02.
  </action>
  <verify>npm run build</verify>
  <done>useLlm hook ready for chat UI integration</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `cd mcp-server && uv pip install ollama` succeeds
- [ ] Python llm module imports without error
- [ ] `cd src-tauri && cargo check` passes
- [ ] `npm run build` succeeds
- [ ] useLlm hook exports correctly
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- LLM integration handles "Ollama not running" gracefully
</success_criteria>

<output>
After completion, create `.planning/phases/05-local-llm-integration/05-01-SUMMARY.md`
</output>
