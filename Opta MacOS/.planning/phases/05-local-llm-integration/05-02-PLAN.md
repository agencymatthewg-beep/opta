---
phase: 05-local-llm-integration
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified: [src/components/ChatInterface.tsx, src/components/ChatMessage.tsx, src/components/ChatInput.tsx, src/pages/Dashboard.tsx, src/hooks/useLlm.ts]
autonomous: true
---

<objective>
Create chat interface with streaming responses for AI assistant.

Purpose: Allow users to ask optimization questions and get real-time streamed responses.
Output: ChatInterface component with message history, streaming display, and input.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Prior plan (Ollama setup):
@.planning/phases/05-local-llm-integration/05-01-PLAN.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ChatMessage and ChatInput components</name>
  <files>src/components/ChatMessage.tsx, src/components/ChatInput.tsx</files>
  <action>
1. Create `ChatMessage.tsx`:
   - Display single message bubble
   - Props: role ("user" | "assistant"), content, isStreaming
   - User messages: right-aligned, primary color background
   - Assistant messages: left-aligned, card background
   - Streaming indicator: animated dots when isStreaming=true
   - Render markdown in assistant messages (use simple parsing or just preserve whitespace)

2. Create `ChatInput.tsx`:
   - Text input with send button
   - Props: onSend, disabled, placeholder
   - Enter to send, Shift+Enter for newline
   - Auto-resize textarea
   - Send button with arrow icon
   - Disabled state while waiting for response

3. Styling (Tailwind + shadcn):
   - ChatMessage: rounded bubbles with subtle shadows
   - ChatInput: dark input with glow on focus
   - Futuristic but clean aesthetic
  </action>
  <verify>npm run build</verify>
  <done>ChatMessage and ChatInput components created</done>
</task>

<task type="auto">
  <name>Task 2: Create ChatInterface container component</name>
  <files>src/components/ChatInterface.tsx, src/hooks/useLlm.ts</files>
  <action>
1. Create `ChatInterface.tsx`:
   - Container for chat messages and input
   - Uses useLlm hook for sending messages
   - Message history stored in state
   - Auto-scroll to bottom on new messages
   - Shows LLM status (connected/disconnected)

2. Layout:
   - Header: "AI Assistant" with status indicator
   - Message area: ScrollArea with ChatMessage components
   - Input area: ChatInput at bottom

3. State management:
   ```typescript
   const [messages, setMessages] = useState<ChatMessage[]>([]);
   const [isTyping, setIsTyping] = useState(false);

   const handleSend = async (content: string) => {
     // Add user message
     setMessages(prev => [...prev, { role: "user", content }]);
     setIsTyping(true);

     // Get response
     const response = await sendMessage(content);

     // Add assistant message
     setMessages(prev => [...prev, { role: "assistant", content: response }]);
     setIsTyping(false);
   };
   ```

4. Error handling:
   - Show inline error if Ollama not running
   - Retry button to check status again
   - Helpful message: "Start Ollama to enable AI features"

Note: True streaming requires WebSocket/SSE which is complex for MVP.
For now, show typing indicator while waiting, display full response when done.
  </action>
  <verify>npm run build</verify>
  <done>ChatInterface component displays messages and handles send</done>
</task>

<task type="auto">
  <name>Task 3: Integrate ChatInterface into Dashboard</name>
  <files>src/pages/Dashboard.tsx</files>
  <action>
1. Add ChatInterface to Dashboard:
   - Place in right column or collapsible panel
   - Can be minimized/expanded
   - Maintains state across minimize

2. Layout options (choose based on space):
   Option A: Side panel (if screen wide enough)
   Option B: Collapsible drawer from right
   Option C: Tab alongside other content

3. Implementation:
   - Add toggle button to show/hide chat
   - Persist open/closed state in localStorage
   - Chat icon with notification badge when minimized

4. Initial message:
   - Pre-populate with welcome: "Hi! I'm your optimization assistant. Ask me about improving your PC's performance."

Keep chat accessible but not overwhelming the main dashboard content.
  </action>
  <verify>npm run dev (chat interface visible and functional on Dashboard)</verify>
  <done>ChatInterface integrated into Dashboard with toggle</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm run build` succeeds
- [ ] ChatMessage renders user and assistant messages correctly
- [ ] ChatInput accepts text and triggers send
- [ ] ChatInterface displays message history
- [ ] Dashboard shows chat toggle/panel
- [ ] Error state shows when Ollama not running
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- Chat interface is usable and matches design system
</success_criteria>

<output>
After completion, create `.planning/phases/05-local-llm-integration/05-02-SUMMARY.md`
</output>
