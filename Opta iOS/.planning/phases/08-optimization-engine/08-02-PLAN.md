# Plan 08-02: Before/After Benchmarking System

## Overview

Create a benchmarking system that captures system performance metrics before and after applying optimizations. This provides measurable, verifiable proof that optimizations actually work.

## Dependencies

**Required:**
- Phase 08-01 complete (optimization action framework)
- `telemetry.py` with CPU/GPU/memory monitoring
- Existing telemetry hooks and types

**Provides:**
- `Benchmark` struct with before/after metrics
- `start_benchmark` / `end_benchmark` commands
- `get_benchmark_results` command
- `BenchmarkResult` TypeScript type
- `useBenchmark` React hook

## Tasks

### Task 1: Create benchmarking module in Python

**File:** `mcp-server/src/opta_mcp/benchmark.py`

```python
"""
Benchmarking system for measuring optimization effectiveness.
"""
import json
import time
import statistics
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict, field
from .telemetry import get_system_snapshot

# Benchmark storage
BENCHMARK_DIR = Path.home() / ".opta" / "benchmarks"

@dataclass
class BenchmarkSample:
    """Single benchmark sample."""
    timestamp: float
    cpu_percent: float
    memory_percent: float
    gpu_percent: Optional[float]
    gpu_temp: Optional[float]
    cpu_temp: Optional[float]

@dataclass
class BenchmarkMetrics:
    """Aggregated benchmark metrics."""
    cpu_avg: float
    cpu_max: float
    memory_avg: float
    memory_max: float
    gpu_avg: Optional[float]
    gpu_max: Optional[float]
    gpu_temp_avg: Optional[float]
    sample_count: int
    duration_seconds: float

@dataclass
class BenchmarkResult:
    """Complete benchmark result with before/after comparison."""
    benchmark_id: str
    game_id: str
    game_name: str
    started_at: float
    completed_at: Optional[float]
    status: str  # "running", "completed", "failed"
    before_metrics: Optional[BenchmarkMetrics]
    after_metrics: Optional[BenchmarkMetrics]
    improvement: Optional[Dict[str, float]]  # Calculated improvements

@dataclass
class ActiveBenchmark:
    """Currently running benchmark."""
    benchmark_id: str
    game_id: str
    game_name: str
    phase: str  # "before" or "after"
    started_at: float
    samples: List[BenchmarkSample] = field(default_factory=list)

# Active benchmark storage (in-memory)
_active_benchmarks: Dict[str, ActiveBenchmark] = {}

def start_benchmark(game_id: str, game_name: str, phase: str = "before") -> Dict:
    """
    Start a benchmark session.

    Args:
        game_id: The game ID being benchmarked
        game_name: Display name of the game
        phase: "before" or "after" optimization

    Returns:
        Benchmark session info
    """
    benchmark_id = f"{game_id}_{int(time.time())}"

    benchmark = ActiveBenchmark(
        benchmark_id=benchmark_id,
        game_id=game_id,
        game_name=game_name,
        phase=phase,
        started_at=time.time(),
        samples=[]
    )

    _active_benchmarks[benchmark_id] = benchmark

    return {
        "benchmark_id": benchmark_id,
        "game_id": game_id,
        "phase": phase,
        "started_at": benchmark.started_at,
        "status": "running"
    }

def capture_sample(benchmark_id: str) -> Optional[Dict]:
    """
    Capture a single benchmark sample.

    Should be called periodically (e.g., every 1 second) during benchmark.
    """
    if benchmark_id not in _active_benchmarks:
        return None

    benchmark = _active_benchmarks[benchmark_id]

    try:
        snapshot = get_system_snapshot()

        sample = BenchmarkSample(
            timestamp=time.time(),
            cpu_percent=snapshot.get("cpu", {}).get("percent", 0),
            memory_percent=snapshot.get("memory", {}).get("percent", 0),
            gpu_percent=snapshot.get("gpu", {}).get("percent"),
            gpu_temp=snapshot.get("gpu", {}).get("temperature"),
            cpu_temp=snapshot.get("cpu", {}).get("temperature")
        )

        benchmark.samples.append(sample)

        return asdict(sample)
    except Exception as e:
        return {"error": str(e)}

def end_benchmark(benchmark_id: str) -> Optional[BenchmarkMetrics]:
    """
    End a benchmark session and calculate metrics.
    """
    if benchmark_id not in _active_benchmarks:
        return None

    benchmark = _active_benchmarks[benchmark_id]
    samples = benchmark.samples

    if len(samples) < 2:
        return None

    # Calculate metrics
    cpu_values = [s.cpu_percent for s in samples]
    memory_values = [s.memory_percent for s in samples]
    gpu_values = [s.gpu_percent for s in samples if s.gpu_percent is not None]
    gpu_temps = [s.gpu_temp for s in samples if s.gpu_temp is not None]

    metrics = BenchmarkMetrics(
        cpu_avg=statistics.mean(cpu_values),
        cpu_max=max(cpu_values),
        memory_avg=statistics.mean(memory_values),
        memory_max=max(memory_values),
        gpu_avg=statistics.mean(gpu_values) if gpu_values else None,
        gpu_max=max(gpu_values) if gpu_values else None,
        gpu_temp_avg=statistics.mean(gpu_temps) if gpu_temps else None,
        sample_count=len(samples),
        duration_seconds=samples[-1].timestamp - samples[0].timestamp
    )

    # Save benchmark data
    save_benchmark_data(benchmark, metrics)

    # Clean up
    del _active_benchmarks[benchmark_id]

    return metrics

def save_benchmark_data(benchmark: ActiveBenchmark, metrics: BenchmarkMetrics):
    """Save benchmark data to disk."""
    BENCHMARK_DIR.mkdir(parents=True, exist_ok=True)

    data = {
        "benchmark_id": benchmark.benchmark_id,
        "game_id": benchmark.game_id,
        "game_name": benchmark.game_name,
        "phase": benchmark.phase,
        "started_at": benchmark.started_at,
        "completed_at": time.time(),
        "metrics": asdict(metrics),
        "samples": [asdict(s) for s in benchmark.samples]
    }

    filepath = BENCHMARK_DIR / f"{benchmark.benchmark_id}_{benchmark.phase}.json"
    with open(filepath, 'w') as f:
        json.dump(data, f, indent=2)

def get_benchmark_pair(game_id: str) -> Optional[Dict]:
    """
    Get the most recent before/after benchmark pair for a game.
    """
    if not BENCHMARK_DIR.exists():
        return None

    # Find matching before/after files
    before_files = sorted(BENCHMARK_DIR.glob(f"{game_id}_*_before.json"), reverse=True)
    after_files = sorted(BENCHMARK_DIR.glob(f"{game_id}_*_after.json"), reverse=True)

    if not before_files or not after_files:
        return None

    # Load most recent pair
    try:
        with open(before_files[0], 'r') as f:
            before_data = json.load(f)
        with open(after_files[0], 'r') as f:
            after_data = json.load(f)
    except Exception:
        return None

    before_metrics = before_data.get("metrics", {})
    after_metrics = after_data.get("metrics", {})

    # Calculate improvements
    improvement = {}
    if before_metrics.get("cpu_avg") and after_metrics.get("cpu_avg"):
        improvement["cpu_reduction"] = before_metrics["cpu_avg"] - after_metrics["cpu_avg"]
        improvement["cpu_reduction_percent"] = (improvement["cpu_reduction"] / before_metrics["cpu_avg"]) * 100

    if before_metrics.get("memory_avg") and after_metrics.get("memory_avg"):
        improvement["memory_reduction"] = before_metrics["memory_avg"] - after_metrics["memory_avg"]
        improvement["memory_reduction_percent"] = (improvement["memory_reduction"] / before_metrics["memory_avg"]) * 100

    if before_metrics.get("gpu_temp_avg") and after_metrics.get("gpu_temp_avg"):
        improvement["gpu_temp_reduction"] = before_metrics["gpu_temp_avg"] - after_metrics["gpu_temp_avg"]

    return {
        "game_id": game_id,
        "game_name": before_data.get("game_name", "Unknown"),
        "before": before_metrics,
        "after": after_metrics,
        "improvement": improvement,
        "before_timestamp": before_data.get("started_at"),
        "after_timestamp": after_data.get("started_at")
    }

def get_all_benchmarks() -> List[Dict]:
    """Get summary of all benchmark results."""
    if not BENCHMARK_DIR.exists():
        return []

    results = []
    seen_games = set()

    for filepath in sorted(BENCHMARK_DIR.glob("*_before.json"), reverse=True):
        try:
            with open(filepath, 'r') as f:
                data = json.load(f)

            game_id = data.get("game_id")
            if game_id in seen_games:
                continue
            seen_games.add(game_id)

            pair = get_benchmark_pair(game_id)
            if pair:
                results.append(pair)
        except Exception:
            continue

    return results

def run_quick_benchmark(duration_seconds: int = 10) -> Dict:
    """
    Run a quick system benchmark without game context.

    Useful for baseline system performance measurement.
    """
    samples = []
    start_time = time.time()

    while time.time() - start_time < duration_seconds:
        try:
            snapshot = get_system_snapshot()
            sample = {
                "timestamp": time.time(),
                "cpu_percent": snapshot.get("cpu", {}).get("percent", 0),
                "memory_percent": snapshot.get("memory", {}).get("percent", 0),
                "gpu_percent": snapshot.get("gpu", {}).get("percent"),
            }
            samples.append(sample)
            time.sleep(1)
        except Exception:
            continue

    if len(samples) < 2:
        return {"error": "Insufficient samples collected"}

    cpu_values = [s["cpu_percent"] for s in samples]
    memory_values = [s["memory_percent"] for s in samples]
    gpu_values = [s["gpu_percent"] for s in samples if s["gpu_percent"] is not None]

    return {
        "duration_seconds": duration_seconds,
        "sample_count": len(samples),
        "cpu": {
            "avg": statistics.mean(cpu_values),
            "max": max(cpu_values),
            "min": min(cpu_values)
        },
        "memory": {
            "avg": statistics.mean(memory_values),
            "max": max(memory_values),
            "min": min(memory_values)
        },
        "gpu": {
            "avg": statistics.mean(gpu_values) if gpu_values else None,
            "max": max(gpu_values) if gpu_values else None,
            "min": min(gpu_values) if gpu_values else None
        } if gpu_values else None
    }
```

### Task 2: Add MCP tools for benchmarking

**File:** `mcp-server/src/opta_mcp/server.py`

Add to TOOLS list:

```python
Tool(
    name="start_benchmark",
    description="Start a benchmark session to measure performance",
    inputSchema={
        "type": "object",
        "properties": {
            "game_id": {"type": "string", "description": "Game ID being benchmarked"},
            "game_name": {"type": "string", "description": "Game display name"},
            "phase": {"type": "string", "enum": ["before", "after"], "description": "Benchmark phase"}
        },
        "required": ["game_id", "game_name", "phase"]
    }
),
Tool(
    name="capture_benchmark_sample",
    description="Capture a single benchmark sample (call periodically during benchmark)",
    inputSchema={
        "type": "object",
        "properties": {
            "benchmark_id": {"type": "string", "description": "Active benchmark ID"}
        },
        "required": ["benchmark_id"]
    }
),
Tool(
    name="end_benchmark",
    description="End a benchmark session and get metrics",
    inputSchema={
        "type": "object",
        "properties": {
            "benchmark_id": {"type": "string", "description": "Benchmark ID to end"}
        },
        "required": ["benchmark_id"]
    }
),
Tool(
    name="get_benchmark_results",
    description="Get benchmark results for a game or all games",
    inputSchema={
        "type": "object",
        "properties": {
            "game_id": {"type": "string", "description": "Optional game ID"}
        },
        "required": []
    }
),
Tool(
    name="quick_benchmark",
    description="Run a quick system benchmark without game context",
    inputSchema={
        "type": "object",
        "properties": {
            "duration_seconds": {"type": "integer", "description": "Benchmark duration (default 10)"}
        },
        "required": []
    }
),
```

Add handlers:

```python
elif name == "start_benchmark":
    from opta_mcp.benchmark import start_benchmark
    result = start_benchmark(
        arguments.get("game_id"),
        arguments.get("game_name"),
        arguments.get("phase", "before")
    )
    return [TextContent(type="text", text=json.dumps(result))]

elif name == "capture_benchmark_sample":
    from opta_mcp.benchmark import capture_sample
    result = capture_sample(arguments.get("benchmark_id"))
    return [TextContent(type="text", text=json.dumps(result or {"error": "Benchmark not found"}))]

elif name == "end_benchmark":
    from opta_mcp.benchmark import end_benchmark
    from dataclasses import asdict
    metrics = end_benchmark(arguments.get("benchmark_id"))
    return [TextContent(type="text", text=json.dumps(asdict(metrics) if metrics else {"error": "Benchmark not found or insufficient samples"}))]

elif name == "get_benchmark_results":
    from opta_mcp.benchmark import get_benchmark_pair, get_all_benchmarks
    game_id = arguments.get("game_id")
    if game_id:
        result = get_benchmark_pair(game_id)
    else:
        result = get_all_benchmarks()
    return [TextContent(type="text", text=json.dumps(result or []))]

elif name == "quick_benchmark":
    from opta_mcp.benchmark import run_quick_benchmark
    duration = arguments.get("duration_seconds", 10)
    result = run_quick_benchmark(duration)
    return [TextContent(type="text", text=json.dumps(result))]
```

### Task 3: Create Rust commands for benchmarking

**File:** `src-tauri/src/benchmark.rs`

```rust
use serde::{Deserialize, Serialize};
use std::process::Command;

#[derive(Debug, Serialize, Deserialize)]
pub struct BenchmarkSession {
    pub benchmark_id: String,
    pub game_id: String,
    pub phase: String,
    pub started_at: f64,
    pub status: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct BenchmarkMetrics {
    pub cpu_avg: f64,
    pub cpu_max: f64,
    pub memory_avg: f64,
    pub memory_max: f64,
    pub gpu_avg: Option<f64>,
    pub gpu_max: Option<f64>,
    pub gpu_temp_avg: Option<f64>,
    pub sample_count: u32,
    pub duration_seconds: f64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct BenchmarkComparison {
    pub game_id: String,
    pub game_name: String,
    pub before: Option<BenchmarkMetrics>,
    pub after: Option<BenchmarkMetrics>,
    pub improvement: Option<serde_json::Value>,
    pub before_timestamp: Option<f64>,
    pub after_timestamp: Option<f64>,
}

const START_BENCHMARK_SCRIPT: &str = r#"
import sys
import json
sys.path.insert(0, 'mcp-server/src')
from opta_mcp.benchmark import start_benchmark

game_id = sys.argv[1]
game_name = sys.argv[2]
phase = sys.argv[3]
result = start_benchmark(game_id, game_name, phase)
print(json.dumps(result))
"#;

const CAPTURE_SAMPLE_SCRIPT: &str = r#"
import sys
import json
sys.path.insert(0, 'mcp-server/src')
from opta_mcp.benchmark import capture_sample

benchmark_id = sys.argv[1]
result = capture_sample(benchmark_id)
print(json.dumps(result or {"error": "Not found"}))
"#;

const END_BENCHMARK_SCRIPT: &str = r#"
import sys
import json
sys.path.insert(0, 'mcp-server/src')
from opta_mcp.benchmark import end_benchmark
from dataclasses import asdict

benchmark_id = sys.argv[1]
metrics = end_benchmark(benchmark_id)
print(json.dumps(asdict(metrics) if metrics else {"error": "Failed"}))
"#;

const GET_RESULTS_SCRIPT: &str = r#"
import sys
import json
sys.path.insert(0, 'mcp-server/src')
from opta_mcp.benchmark import get_benchmark_pair, get_all_benchmarks

game_id = sys.argv[1] if len(sys.argv) > 1 and sys.argv[1] != "" else None
if game_id:
    result = get_benchmark_pair(game_id)
else:
    result = get_all_benchmarks()
print(json.dumps(result or []))
"#;

#[tauri::command]
pub async fn start_benchmark(
    game_id: String,
    game_name: String,
    phase: String,
) -> Result<BenchmarkSession, String> {
    let output = Command::new("python3")
        .arg("-c")
        .arg(START_BENCHMARK_SCRIPT)
        .arg(&game_id)
        .arg(&game_name)
        .arg(&phase)
        .current_dir(std::env::current_dir().map_err(|e| e.to_string())?)
        .output()
        .map_err(|e| format!("Failed to execute Python: {}", e))?;

    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        return Err(format!("Python script failed: {}", stderr));
    }

    let stdout = String::from_utf8_lossy(&output.stdout);
    serde_json::from_str(&stdout)
        .map_err(|e| format!("Failed to parse result: {}", e))
}

#[tauri::command]
pub async fn capture_benchmark_sample(benchmark_id: String) -> Result<serde_json::Value, String> {
    let output = Command::new("python3")
        .arg("-c")
        .arg(CAPTURE_SAMPLE_SCRIPT)
        .arg(&benchmark_id)
        .current_dir(std::env::current_dir().map_err(|e| e.to_string())?)
        .output()
        .map_err(|e| format!("Failed to execute Python: {}", e))?;

    let stdout = String::from_utf8_lossy(&output.stdout);
    serde_json::from_str(&stdout)
        .map_err(|e| format!("Failed to parse result: {}", e))
}

#[tauri::command]
pub async fn end_benchmark(benchmark_id: String) -> Result<serde_json::Value, String> {
    let output = Command::new("python3")
        .arg("-c")
        .arg(END_BENCHMARK_SCRIPT)
        .arg(&benchmark_id)
        .current_dir(std::env::current_dir().map_err(|e| e.to_string())?)
        .output()
        .map_err(|e| format!("Failed to execute Python: {}", e))?;

    let stdout = String::from_utf8_lossy(&output.stdout);
    serde_json::from_str(&stdout)
        .map_err(|e| format!("Failed to parse result: {}", e))
}

#[tauri::command]
pub async fn get_benchmark_results(game_id: Option<String>) -> Result<serde_json::Value, String> {
    let output = Command::new("python3")
        .arg("-c")
        .arg(GET_RESULTS_SCRIPT)
        .arg(game_id.unwrap_or_default())
        .current_dir(std::env::current_dir().map_err(|e| e.to_string())?)
        .output()
        .map_err(|e| format!("Failed to execute Python: {}", e))?;

    let stdout = String::from_utf8_lossy(&output.stdout);
    serde_json::from_str(&stdout)
        .map_err(|e| format!("Failed to parse result: {}", e))
}
```

Register in `src-tauri/src/lib.rs`:

```rust
mod benchmark;

// In invoke_handler:
benchmark::start_benchmark,
benchmark::capture_benchmark_sample,
benchmark::end_benchmark,
benchmark::get_benchmark_results,
```

### Task 4: Add TypeScript types and hook

**File:** `src/types/benchmark.ts`

```typescript
/**
 * Benchmarking types.
 */

export interface BenchmarkSession {
  benchmark_id: string;
  game_id: string;
  phase: 'before' | 'after';
  started_at: number;
  status: 'running' | 'completed' | 'failed';
}

export interface BenchmarkMetrics {
  cpu_avg: number;
  cpu_max: number;
  memory_avg: number;
  memory_max: number;
  gpu_avg: number | null;
  gpu_max: number | null;
  gpu_temp_avg: number | null;
  sample_count: number;
  duration_seconds: number;
}

export interface BenchmarkImprovement {
  cpu_reduction?: number;
  cpu_reduction_percent?: number;
  memory_reduction?: number;
  memory_reduction_percent?: number;
  gpu_temp_reduction?: number;
}

export interface BenchmarkComparison {
  game_id: string;
  game_name: string;
  before: BenchmarkMetrics | null;
  after: BenchmarkMetrics | null;
  improvement: BenchmarkImprovement | null;
  before_timestamp: number | null;
  after_timestamp: number | null;
}
```

**File:** `src/hooks/useBenchmark.ts`

```typescript
/**
 * Hook for benchmarking operations.
 */

import { useState, useCallback, useRef, useEffect } from 'react';
import { invoke } from '@tauri-apps/api/core';
import type { BenchmarkSession, BenchmarkComparison } from '../types/benchmark';

export interface UseBenchmarkResult {
  /** Start a benchmark session */
  startBenchmark: (gameId: string, gameName: string, phase: 'before' | 'after') => Promise<BenchmarkSession>;
  /** End the current benchmark and get metrics */
  endBenchmark: () => Promise<void>;
  /** Get benchmark comparison for a game */
  getComparison: (gameId: string) => Promise<BenchmarkComparison | null>;
  /** Get all benchmark results */
  getAllResults: () => Promise<BenchmarkComparison[]>;
  /** Current active benchmark session */
  activeSession: BenchmarkSession | null;
  /** Whether benchmarking is in progress */
  isRunning: boolean;
  /** Sample count for current benchmark */
  sampleCount: number;
  /** Error message */
  error: string | null;
}

export function useBenchmark(): UseBenchmarkResult {
  const [activeSession, setActiveSession] = useState<BenchmarkSession | null>(null);
  const [isRunning, setIsRunning] = useState(false);
  const [sampleCount, setSampleCount] = useState(0);
  const [error, setError] = useState<string | null>(null);

  const samplingIntervalRef = useRef<NodeJS.Timeout | null>(null);

  // Clean up sampling interval on unmount
  useEffect(() => {
    return () => {
      if (samplingIntervalRef.current) {
        clearInterval(samplingIntervalRef.current);
      }
    };
  }, []);

  const startBenchmark = useCallback(async (
    gameId: string,
    gameName: string,
    phase: 'before' | 'after'
  ): Promise<BenchmarkSession> => {
    setError(null);
    try {
      const session = await invoke<BenchmarkSession>('start_benchmark', {
        gameId,
        gameName,
        phase,
      });

      setActiveSession(session);
      setIsRunning(true);
      setSampleCount(0);

      // Start sampling every second
      samplingIntervalRef.current = setInterval(async () => {
        try {
          await invoke('capture_benchmark_sample', {
            benchmarkId: session.benchmark_id,
          });
          setSampleCount(prev => prev + 1);
        } catch (e) {
          console.error('Sample capture failed:', e);
        }
      }, 1000);

      return session;
    } catch (e) {
      const errorMessage = e instanceof Error ? e.message : String(e);
      setError(errorMessage);
      throw new Error(errorMessage);
    }
  }, []);

  const endBenchmark = useCallback(async (): Promise<void> => {
    if (!activeSession) {
      throw new Error('No active benchmark session');
    }

    // Stop sampling
    if (samplingIntervalRef.current) {
      clearInterval(samplingIntervalRef.current);
      samplingIntervalRef.current = null;
    }

    try {
      await invoke('end_benchmark', {
        benchmarkId: activeSession.benchmark_id,
      });

      setActiveSession(null);
      setIsRunning(false);
    } catch (e) {
      const errorMessage = e instanceof Error ? e.message : String(e);
      setError(errorMessage);
      throw new Error(errorMessage);
    }
  }, [activeSession]);

  const getComparison = useCallback(async (gameId: string): Promise<BenchmarkComparison | null> => {
    try {
      const result = await invoke<BenchmarkComparison | null>('get_benchmark_results', {
        gameId,
      });
      return result;
    } catch (e) {
      const errorMessage = e instanceof Error ? e.message : String(e);
      setError(errorMessage);
      return null;
    }
  }, []);

  const getAllResults = useCallback(async (): Promise<BenchmarkComparison[]> => {
    try {
      const result = await invoke<BenchmarkComparison[]>('get_benchmark_results', {
        gameId: null,
      });
      return result || [];
    } catch (e) {
      const errorMessage = e instanceof Error ? e.message : String(e);
      setError(errorMessage);
      return [];
    }
  }, []);

  return {
    startBenchmark,
    endBenchmark,
    getComparison,
    getAllResults,
    activeSession,
    isRunning,
    sampleCount,
    error,
  };
}

export default useBenchmark;
```

## Success Criteria

- [ ] `benchmark.py` module created with start/capture/end/results functions
- [ ] MCP tools registered for benchmark operations
- [ ] Rust commands compile and invoke Python correctly
- [ ] TypeScript types and hook created with auto-sampling
- [ ] Test: `python -c "from opta_mcp.benchmark import run_quick_benchmark; print(run_quick_benchmark(5))"`
- [ ] `cargo build` succeeds
- [ ] `npm run build` succeeds

## Verification

```bash
# Test Python module
cd /Users/matthewbyrden/Documents/Opta
python3 -c "
import sys
sys.path.insert(0, 'mcp-server/src')
from opta_mcp.benchmark import run_quick_benchmark

result = run_quick_benchmark(5)
print(f'CPU avg: {result[\"cpu\"][\"avg\"]:.1f}%')
print(f'Memory avg: {result[\"memory\"][\"avg\"]:.1f}%')
print(f'Samples: {result[\"sample_count\"]}')
"

# Build checks
cargo build
npm run build
```

## Estimated Complexity

- **Scope:** Medium (new module + 4 commands + types + hook with auto-sampling)
- **Risk:** Low (builds on existing telemetry patterns)
- **Duration:** ~15 min
