[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "opta-lmx"
version = "0.1.0"
description = "Private AI inference engine for Apple Silicon â€” MLX-native, OpenAI-compatible"
requires-python = ">=3.12"
license = "MIT"
authors = [
    { name = "Matthew Byrden" },
]

dependencies = [
    "vllm-mlx>=0.2.6",
    "fastapi>=0.115.0",
    "uvicorn[standard]>=0.30.0",
    "pydantic>=2.7",
    "pydantic-settings>=2.0",
    "pyyaml>=6.0",
    "psutil>=5.9",
    "huggingface-hub>=0.25",
    "structlog>=25.0",
    "tqdm>=4.60",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0",
    "pytest-asyncio>=0.23,<2.0",
    "httpx>=0.27",
    "ruff>=0.8",
    "mypy>=1.0",
    "openai>=1.0",
]
embeddings = [
    "mlx-embeddings>=0.1.0",
]
gguf = [
    "llama-cpp-python>=0.3.0",
]

[project.scripts]
opta-lmx = "opta_lmx.main:cli"

[tool.hatch.build.targets.wheel]
packages = ["src/opta_lmx"]

[tool.pytest.ini_options]
asyncio_mode = "auto"
testpaths = ["tests"]

[tool.ruff]
line-length = 100
target-version = "py312"

[tool.ruff.lint]
select = ["E", "F", "I", "N", "W", "UP", "B", "SIM", "RUF", "ASYNC", "T20"]

[tool.mypy]
python_version = "3.12"
strict = true

[[tool.mypy.overrides]]
module = [
    "llama_cpp",
    "llama_cpp.*",
    "vllm_mlx",
    "vllm_mlx.*",
    "huggingface_hub",
    "huggingface_hub.*",
]
ignore_missing_imports = true
