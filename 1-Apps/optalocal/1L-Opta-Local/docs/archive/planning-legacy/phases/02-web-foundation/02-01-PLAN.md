---
phase: 02-web-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - 1-Apps/1L-Opta-Local/web/package.json
  - 1-Apps/1L-Opta-Local/web/src/hooks/useChatStream.ts
  - 1-Apps/1L-Opta-Local/web/src/hooks/useScrollAnchor.ts
  - 1-Apps/1L-Opta-Local/web/src/components/chat/ChatMessage.tsx
  - 1-Apps/1L-Opta-Local/web/src/components/chat/ChatInput.tsx
  - 1-Apps/1L-Opta-Local/web/src/components/chat/ChatContainer.tsx
  - 1-Apps/1L-Opta-Local/web/src/app/chat/page.tsx
autonomous: true
status: review
---

<objective>
Build the streaming chat engine and message UI — the core "chat with your local AI" experience.

Purpose: Create the primary interaction surface: a streaming chat interface that connects to LMX via the Phase 1 LMXClient, renders markdown with syntax highlighting, and handles auto-scroll during token streaming.
Output: A working /chat page with streaming responses, markdown rendering via Streamdown, and scroll-anchor behavior.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-web-foundation/02-RESEARCH.md
@.planning/phases/01-web-project-setup/01-02-SUMMARY.md

# Phase 1 source files (LMXClient, types, connection):
@1-Apps/1L-Opta-Local/web/src/lib/lmx-client.ts
@1-Apps/1L-Opta-Local/web/src/lib/connection.ts
@1-Apps/1L-Opta-Local/web/src/types/lmx.ts
@1-Apps/1L-Opta-Local/web/src/app/globals.css
@1-Apps/1L-Opta-Local/web/src/app/layout.tsx

# Design system reference:
@1-Apps/1L-Opta-Local/web/CLAUDE.md
@1-Apps/1L-Opta-Local/SHARED.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install Phase 2 dependencies and create streaming chat hooks</name>
  <files>
    1-Apps/1L-Opta-Local/web/package.json,
    1-Apps/1L-Opta-Local/web/src/hooks/useChatStream.ts,
    1-Apps/1L-Opta-Local/web/src/hooks/useScrollAnchor.ts
  </files>
  <action>
    1. Install Phase 2 dependencies:
       ```bash
       cd 1-Apps/1L-Opta-Local/web && pnpm add streamdown @streamdown/code idb-keyval @radix-ui/react-select @radix-ui/react-tooltip
       ```

    2. Create `src/hooks/useChatStream.ts` — a 'use client' hook wrapping LMXClient.streamChat():
       - Accept LMXClient instance, model string
       - Maintain messages state as ChatMessage[]
       - sendMessage(content): adds user message optimistically, creates empty assistant message, streams tokens via async generator
       - Wrap token-append setState calls in React 19 startTransition() so streaming doesn't block user input
       - Track isStreaming boolean state
       - Provide stop() via AbortController ref
       - Return { messages, setMessages, isStreaming, isPending, sendMessage, stop }
       - See 02-RESEARCH.md Pattern 1 for the exact implementation pattern

    3. Create `src/hooks/useScrollAnchor.ts` — a 'use client' hook for auto-scroll during streaming:
       - Uses IntersectionObserver on an anchor div at the bottom of the message list
       - isAtBottom: true when anchor is visible
       - autoScroll(): scrolls to anchor via requestAnimationFrame only when isAtBottom is true
       - scrollToBottom(smooth?): manual scroll-to-bottom
       - showScrollButton: true when user has scrolled up
       - Return { containerRef, anchorRef, isAtBottom, showScrollButton, scrollToBottom, autoScroll }
       - See 02-RESEARCH.md Pattern 3 for the exact implementation pattern
  </action>
  <verify>pnpm run build (from web dir) passes with no TypeScript errors; streamdown and @streamdown/code resolve correctly</verify>
  <done>Phase 2 dependencies installed; useChatStream hook streams tokens with startTransition; useScrollAnchor tracks scroll position with IntersectionObserver</done>
</task>

<task type="auto">
  <name>Task 2: Create chat message components and /chat page</name>
  <files>
    1-Apps/1L-Opta-Local/web/src/components/chat/ChatMessage.tsx,
    1-Apps/1L-Opta-Local/web/src/components/chat/ChatInput.tsx,
    1-Apps/1L-Opta-Local/web/src/components/chat/ChatContainer.tsx,
    1-Apps/1L-Opta-Local/web/src/app/chat/page.tsx
  </files>
  <action>
    ALL UI components MUST follow Opta glass design system from CLAUDE.md: cn() for classes, CSS variables for colors, Framer Motion for animations, Lucide icons, Sora font, dark-mode-only.

    1. Create `src/components/chat/ChatMessage.tsx` ('use client'):
       - Props: { content: string, role: 'user' | 'assistant', isStreaming?: boolean }
       - User messages: glass-subtle panel, right-aligned (ml-12), white-space pre-wrap text
       - Assistant messages: left-aligned (mr-12), rendered with Streamdown component
       - Import Streamdown from 'streamdown' and createCodePlugin from '@streamdown/code'
       - Create optaCode plugin with github-dark-default theme for both light/dark (OLED-only)
       - Pass caret='block' and isAnimating={isStreaming} to Streamdown for streaming indicator
       - See 02-RESEARCH.md Pattern 2 for Streamdown integration

    2. Create `src/components/chat/ChatInput.tsx` ('use client'):
       - Multiline textarea with glass-subtle background
       - Submit on Enter (Shift+Enter for newline)
       - Disabled state when isStreaming (show stop button instead via Lucide Square icon)
       - Props: { onSend: (content: string) => void, onStop: () => void, isStreaming: boolean, disabled?: boolean }
       - Send icon from Lucide (SendHorizontal), stop icon (Square)
       - Auto-focus on mount, auto-resize textarea height

    3. Create `src/components/chat/ChatContainer.tsx` ('use client'):
       - Integrates useChatStream + useScrollAnchor + ChatMessage + ChatInput
       - Accepts model prop (string) for the selected model
       - Creates LMXClient from ConnectionSettings via createClient()
       - Renders scrollable message list with anchor div at bottom
       - Shows "scroll to bottom" button when showScrollButton is true (Lucide ChevronDown icon)
       - Calls autoScroll() when messages change during streaming
       - Empty state: centered prompt suggestions or welcome message

    4. Create `src/app/chat/page.tsx`:
       - 'use client' page wrapping ChatContainer
       - Hardcode a default model for now (will be replaced by model picker in 02-02)
       - Full-height layout: flex column, message area fills remaining space, input at bottom
       - Page title: "Chat — Opta Local"
  </action>
  <verify>pnpm run build passes; dev server shows /chat page; typing a message and pressing Enter sends it (will fail to connect to LMX if server is down, but UI should render without errors)</verify>
  <done>ChatMessage renders user/assistant messages with Streamdown markdown; ChatInput handles Enter/Shift+Enter/stop; ChatContainer integrates scroll anchor with streaming; /chat page is navigable</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `cd 1-Apps/1L-Opta-Local/web && pnpm run build` succeeds without errors
- [ ] `/chat` page renders in dev server (pnpm dev)
- [ ] ChatMessage renders markdown with syntax highlighting for code blocks
- [ ] Chat input handles Enter to send, Shift+Enter for newline
- [ ] Scroll behavior: auto-scrolls during streaming, stops when user scrolls up
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No TypeScript errors
- Streaming chat UI renders and connects to LMXClient
- Markdown rendering works with code highlighting
</success_criteria>

<output>
After completion, create `.planning/phases/02-web-foundation/02-01-SUMMARY.md`
</output>
