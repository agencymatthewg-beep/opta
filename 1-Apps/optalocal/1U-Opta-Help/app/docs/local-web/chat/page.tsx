"use client";

import { Breadcrumb } from "@/components/layout/Breadcrumb";
import { PrevNextNav } from "@/components/docs/PrevNextNav";
import { TableOfContents } from "@/components/docs/TableOfContents";
import { getPrevNext } from "@/lib/content";
import { Callout } from "@/components/docs/Callout";
import { CodeBlock } from "@/components/docs/CodeBlock";

const tocItems = [
  { id: "chat-interface", title: "Chat Interface", level: 2 as const },
  { id: "model-picker", title: "Model Picker", level: 2 as const },
  { id: "streaming-responses", title: "Streaming Responses", level: 2 as const },
  { id: "tool-execution", title: "Tool Execution Feedback", level: 2 as const },
  { id: "session-continuity", title: "Session Continuity", level: 2 as const },
  { id: "markdown-rendering", title: "Markdown Rendering", level: 2 as const },
];

export default function LocalWebChatPage() {
  const { prev, next } = getPrevNext("/docs/local-web/chat/");

  return (
    <>
      <Breadcrumb
        items={[
          { label: "Local Web", href: "/docs/local-web/" },
          { label: "Chat" },
        ]}
      />

      <div className="flex gap-10">
        <div className="flex-1 min-w-0 prose-opta">
          <h1>Chat</h1>
          <p className="lead">
            The Opta Local Web chat interface provides a full streaming
            conversation experience in your browser, backed by the same
            models running on your LMX inference server.
          </p>

          <h2 id="chat-interface">Chat Interface</h2>
          <p>
            The chat view presents a familiar conversational layout: your
            messages on one side, model responses on the other, rendered
            in a scrolling message list. Messages are composed in an input
            area at the bottom of the screen with keyboard shortcuts for
            submission.
          </p>
          <p>
            The interface supports multi-turn conversations with full
            context retention. Each message is displayed with the model
            identity and timestamp, and the entire conversation history
            is maintained in the browser session.
          </p>

          <h2 id="model-picker">Model Picker</h2>
          <p>
            Before starting a conversation (or at any point during one),
            you can select which loaded model to use via the model picker.
            The picker shows only models currently loaded in LMX memory --
            it pulls the active model list from the same SSE stream that
            powers the dashboard.
          </p>
          <p>
            Selecting a different model mid-conversation switches inference
            to that model for subsequent messages. Previous messages in the
            conversation are preserved and re-sent as context to the new
            model.
          </p>

          <Callout variant="tip" title="Model switching">
            Switching models mid-conversation is useful for comparing
            response quality. Ask the same question to different models
            within a single session to evaluate their capabilities
            side by side.
          </Callout>

          <h2 id="streaming-responses">Streaming Responses</h2>
          <p>
            Responses stream token by token as they are generated by LMX.
            The web client connects to the standard OpenAI-compatible
            streaming endpoint and renders tokens incrementally as they
            arrive.
          </p>

          <CodeBlock
            language="text"
            filename="Streaming Request"
            code={`POST /v1/chat/completions
Content-Type: application/json

{
  "model": "qwen3-72b",
  "messages": [
    { "role": "user", "content": "Explain unified memory architecture" }
  ],
  "stream": true
}`}
          />

          <p>
            During streaming, a typing indicator and token counter are
            visible. The response text renders with progressive markdown
            formatting -- headings, code blocks, and lists appear as soon
            as enough tokens have been received to parse them.
          </p>

          <h2 id="tool-execution">Tool Execution Feedback</h2>
          <p>
            When the model requests tool execution (file reads, web
            searches, code execution), the chat interface displays tool
            call cards inline with the conversation. Each card shows:
          </p>
          <ul>
            <li><strong>Tool name</strong> -- which tool the model invoked</li>
            <li><strong>Arguments</strong> -- the parameters passed to the tool</li>
            <li><strong>Result</strong> -- the output returned to the model</li>
            <li><strong>Duration</strong> -- how long the tool execution took</li>
          </ul>
          <p>
            Tool cards are collapsible -- click to expand or collapse the
            details. This keeps the conversation readable while still
            providing full transparency into what tools the model used and
            what data they returned.
          </p>

          <h2 id="session-continuity">Session Continuity</h2>
          <p>
            Sessions started in the Opta CLI can be continued in the web
            interface, and vice versa. The daemon maintains session state
            independently of the client, so you can:
          </p>
          <ul>
            <li>Start a coding conversation in your terminal with <code>opta chat</code></li>
            <li>Switch to the web dashboard to continue the same session visually</li>
            <li>Return to the CLI and pick up where you left off</li>
          </ul>
          <p>
            Session continuity works because both the CLI and web dashboard
            connect to the same daemon session store. The session ID is the
            key -- any client that knows the session ID can resume the
            conversation.
          </p>

          <Callout variant="info" title="Session persistence">
            Session data is persisted to disk by the daemon. Conversations
            survive daemon restarts and can be resumed hours or days later
            with full context.
          </Callout>

          <h2 id="markdown-rendering">Markdown Rendering</h2>
          <p>
            Model responses are rendered with full markdown support,
            including:
          </p>
          <ul>
            <li>Headings (h1 through h6)</li>
            <li>Fenced code blocks with syntax highlighting</li>
            <li>Inline code spans</li>
            <li>Bold, italic, and strikethrough text</li>
            <li>Ordered and unordered lists</li>
            <li>Blockquotes</li>
            <li>Tables</li>
            <li>Links (opening in new tabs)</li>
          </ul>
          <p>
            Code blocks include a copy button for extracting snippets. The
            markdown renderer matches the Opta design system -- dark
            backgrounds for code blocks, violet accent for links, and
            consistent typography throughout.
          </p>

          <PrevNextNav prev={prev} next={next} />
        </div>

        <TableOfContents items={tocItems} />
      </div>
    </>
  );
}
