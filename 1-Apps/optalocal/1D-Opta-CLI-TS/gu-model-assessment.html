<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Opta LMX Model Assessment ‚Äî Step-3.5-Flash & Qwen3.5</title>
  <link href="https://fonts.googleapis.com/css2?family=Sora:wght@300;400;500;600;700&display=swap" rel="stylesheet" />
  <style>
    *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
    :root {
      --void: #09090b; --surface: #18181b; --elevated: #27272a; --border: #3f3f46;
      --text-primary: #fafafa; --text-secondary: #a1a1aa; --text-muted: #52525b;
      --primary: #8b5cf6; --primary-glow: #a855f7;
      --neon-blue: #3b82f6; --neon-green: #22c55e; --neon-amber: #f59e0b;
      --neon-red: #ef4444; --neon-cyan: #06b6d4; --neon-pink: #ec4899;
    }
    body { background: var(--void); color: var(--text-primary); font-family: system-ui, -apple-system, sans-serif; min-height: 100vh; overflow-x: hidden; }
    body::before {
      content: ''; position: fixed; inset: 0; pointer-events: none; z-index: 0;
      background:
        radial-gradient(ellipse 70% 50% at 15% 0%, rgba(139,92,246,0.1) 0%, transparent 60%),
        radial-gradient(ellipse 50% 40% at 85% 100%, rgba(59,130,246,0.07) 0%, transparent 60%),
        radial-gradient(ellipse 30% 30% at 50% 50%, rgba(6,182,212,0.03) 0%, transparent 50%);
    }
    .container { max-width: 1300px; margin: 0 auto; padding: 48px 32px 80px; position: relative; z-index: 1; }

    /* Hero */
    .hero { text-align: center; margin-bottom: 56px; }
    .hero-badge {
      display: inline-flex; align-items: center; gap: 8px; padding: 6px 16px;
      border-radius: 100px; border: 1px solid rgba(139,92,246,0.4);
      background: rgba(139,92,246,0.1); font-size: 11px; font-family: 'Sora', sans-serif;
      font-weight: 600; letter-spacing: 0.12em; text-transform: uppercase;
      color: var(--primary-glow); margin-bottom: 24px;
    }
    .hero-badge::before {
      content: ''; width: 6px; height: 6px; border-radius: 50%;
      background: var(--neon-cyan); box-shadow: 0 0 8px var(--neon-cyan);
      animation: pulse 2s ease-in-out infinite;
    }
    @keyframes pulse { 0%,100%{opacity:1} 50%{opacity:0.4} }
    h1 {
      font-family: 'Sora', sans-serif; font-size: clamp(28px, 4vw, 52px);
      font-weight: 700; line-height: 1.1;
      background: linear-gradient(135deg, #fafafa 0%, #c4b5fd 40%, #67e8f9 80%, #818cf8 100%);
      -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text;
      margin-bottom: 16px;
    }
    .hero-sub { font-size: 17px; color: var(--text-secondary); max-width: 620px; margin: 0 auto; line-height: 1.6; }

    /* Verdict cards */
    .verdict-row { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 48px; }
    .verdict-card {
      border-radius: 20px; padding: 32px; position: relative; overflow: hidden;
      border: 1px solid rgba(255,255,255,0.08);
      box-shadow: 0 24px 80px rgba(0,0,0,0.4);
    }
    .verdict-card::before {
      content: ''; position: absolute; top: 0; left: 0; right: 0;
      height: 1px; background: linear-gradient(90deg, transparent, rgba(255,255,255,0.15), transparent);
    }
    .vc-green { background: linear-gradient(135deg, rgba(24,24,27,0.95), rgba(17,24,20,0.9)); border-color: rgba(34,197,94,0.25); }
    .vc-amber { background: linear-gradient(135deg, rgba(24,24,27,0.95), rgba(24,20,10,0.9)); border-color: rgba(245,158,11,0.25); }

    .vc-accent {
      position: absolute; top: 0; left: 0; right: 0; height: 3px;
    }
    .vc-green .vc-accent { background: linear-gradient(90deg, var(--neon-green), #4ade80); box-shadow: 0 0 20px rgba(34,197,94,0.5); }
    .vc-amber .vc-accent { background: linear-gradient(90deg, var(--neon-amber), #fbbf24); box-shadow: 0 0 20px rgba(245,158,11,0.5); }

    .vc-verdict-badge {
      display: inline-flex; align-items: center; gap: 6px; padding: 4px 12px;
      border-radius: 100px; font-size: 10px; font-weight: 700;
      letter-spacing: 0.1em; text-transform: uppercase; margin-bottom: 16px;
    }
    .vc-verdict-badge::before { content: ''; width: 6px; height: 6px; border-radius: 50%; }
    .badge-yes { background: rgba(34,197,94,0.12); border: 1px solid rgba(34,197,94,0.35); color: var(--neon-green); }
    .badge-yes::before { background: var(--neon-green); box-shadow: 0 0 8px var(--neon-green); }
    .badge-marginal { background: rgba(245,158,11,0.12); border: 1px solid rgba(245,158,11,0.35); color: var(--neon-amber); }
    .badge-marginal::before { background: var(--neon-amber); box-shadow: 0 0 8px var(--neon-amber); animation: pulse 1.5s ease-in-out infinite; }

    .vc-model-name { font-family: 'Sora', sans-serif; font-size: 22px; font-weight: 700; color: var(--text-primary); margin-bottom: 4px; }
    .vc-model-sub { font-size: 13px; color: var(--text-muted); margin-bottom: 20px; }

    .vc-stats { display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin-bottom: 20px; }
    .vc-stat { background: rgba(255,255,255,0.04); border: 1px solid rgba(255,255,255,0.06); border-radius: 10px; padding: 12px; text-align: center; }
    .vc-stat-val { font-family: 'Sora', sans-serif; font-size: 18px; font-weight: 700; color: var(--text-primary); line-height: 1; margin-bottom: 4px; }
    .vc-stat-label { font-size: 10px; color: var(--text-muted); letter-spacing: 0.06em; }

    .vc-reasons { display: flex; flex-direction: column; gap: 8px; }
    .vc-reason { display: flex; gap: 10px; align-items: flex-start; font-size: 12.5px; color: var(--text-secondary); line-height: 1.5; }
    .vc-reason-icon { flex-shrink: 0; font-size: 14px; margin-top: 1px; }
    .vc-reason strong { color: var(--text-primary); }

    /* Divider */
    .divider { height: 1px; background: linear-gradient(90deg, transparent, rgba(139,92,246,0.4), rgba(6,182,212,0.3), transparent); margin: 40px 0; position: relative; }
    .divider::after { content: ''; position: absolute; inset: -1px; background: inherit; filter: blur(4px); opacity: 0.5; }

    .section-label { font-family: 'Sora', sans-serif; font-size: 11px; font-weight: 600; letter-spacing: 0.15em; text-transform: uppercase; color: var(--primary); margin-bottom: 8px; }
    h2 { font-family: 'Sora', sans-serif; font-size: 26px; font-weight: 600; color: var(--text-primary); margin-bottom: 28px; display: flex; align-items: center; gap: 12px; }
    h2 .glow-line { flex: 1; height: 1px; background: linear-gradient(90deg, rgba(139,92,246,0.4), transparent); }

    /* Spec comparison */
    .spec-grid { display: grid; grid-template-columns: 200px 1fr 1fr; gap: 0; border-radius: 16px; overflow: hidden; border: 1px solid rgba(255,255,255,0.07); margin-bottom: 40px; }
    .spec-header { background: rgba(39,39,42,0.9); }
    .spec-header-cell { padding: 16px 20px; font-family: 'Sora', sans-serif; font-size: 12px; font-weight: 700; letter-spacing: 0.06em; color: var(--text-muted); text-transform: uppercase; border-bottom: 1px solid rgba(255,255,255,0.07); }
    .spec-header-cell.step { color: var(--neon-green); }
    .spec-header-cell.qwen { color: var(--neon-amber); }
    .spec-row { display: contents; }
    .spec-row:hover > div { background: rgba(255,255,255,0.02); }
    .spec-label { background: rgba(24,24,27,0.8); padding: 14px 20px; font-size: 12px; font-weight: 600; color: var(--text-muted); border-bottom: 1px solid rgba(255,255,255,0.04); display: flex; align-items: center; }
    .spec-val { background: rgba(24,24,27,0.6); padding: 14px 20px; font-size: 13px; color: var(--text-secondary); border-bottom: 1px solid rgba(255,255,255,0.04); display: flex; align-items: center; gap: 8px; }
    .spec-val.highlight { color: var(--text-primary); font-weight: 500; }
    .tag-good { display: inline-block; padding: 2px 7px; border-radius: 4px; font-size: 10px; font-weight: 600; background: rgba(34,197,94,0.1); color: var(--neon-green); border: 1px solid rgba(34,197,94,0.25); }
    .tag-warn { display: inline-block; padding: 2px 7px; border-radius: 4px; font-size: 10px; font-weight: 600; background: rgba(245,158,11,0.1); color: var(--neon-amber); border: 1px solid rgba(245,158,11,0.25); }
    .tag-info { display: inline-block; padding: 2px 7px; border-radius: 4px; font-size: 10px; font-weight: 600; background: rgba(59,130,246,0.1); color: var(--neon-blue); border: 1px solid rgba(59,130,246,0.25); }

    /* Memory viz */
    .memory-section { margin-bottom: 40px; }
    .mem-card { background: rgba(24,24,27,0.8); border: 1px solid rgba(255,255,255,0.08); border-radius: 16px; padding: 24px; margin-bottom: 16px; position: relative; overflow: hidden; }
    .mem-card::before { content: ''; position: absolute; top: 0; left: 0; right: 0; height: 1px; background: linear-gradient(90deg, transparent, rgba(255,255,255,0.1), transparent); }
    .mem-title { font-family: 'Sora', sans-serif; font-size: 14px; font-weight: 600; color: var(--text-primary); margin-bottom: 16px; display: flex; align-items: center; justify-content: space-between; }
    .mem-bar-wrap { height: 12px; background: rgba(255,255,255,0.05); border-radius: 6px; overflow: hidden; margin-bottom: 10px; position: relative; }
    .mem-bar { height: 100%; border-radius: 6px; position: relative; transition: width 0.6s ease; display: flex; align-items: center; }
    .mem-bar-glow { position: absolute; inset: 0; border-radius: 6px; opacity: 0.4; filter: blur(4px); }
    .mem-labels { display: flex; justify-content: space-between; font-size: 11px; color: var(--text-muted); }
    .mem-legend { display: flex; gap: 20px; flex-wrap: wrap; margin-top: 12px; }
    .mem-leg-item { display: flex; align-items: center; gap: 6px; font-size: 11px; color: var(--text-secondary); }
    .mem-dot { width: 10px; height: 10px; border-radius: 2px; flex-shrink: 0; }

    /* Opta CLI integration */
    .cli-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 16px; margin-bottom: 40px; }
    .cli-card { background: rgba(24,24,27,0.8); border: 1px solid rgba(255,255,255,0.08); border-radius: 14px; padding: 20px; position: relative; overflow: hidden; }
    .cli-card::before { content: ''; position: absolute; top: 0; left: 0; right: 0; height: 1px; background: linear-gradient(90deg, transparent, rgba(255,255,255,0.1), transparent); }
    .cli-card-title { font-family: 'Sora', sans-serif; font-size: 13px; font-weight: 600; color: var(--primary); margin-bottom: 12px; }
    .cli-item { display: flex; gap: 10px; align-items: flex-start; padding: 8px 0; border-bottom: 1px solid rgba(255,255,255,0.04); font-size: 12.5px; color: var(--text-secondary); line-height: 1.5; }
    .cli-item:last-child { border-bottom: none; }
    .cli-icon { font-size: 14px; flex-shrink: 0; margin-top: 1px; }
    .cli-item strong { color: var(--text-primary); }

    code { font-family: 'Menlo','Monaco',monospace; font-size: 11px; background: rgba(139,92,246,0.1); border: 1px solid rgba(139,92,246,0.2); border-radius: 4px; padding: 1px 5px; color: #c4b5fd; }

    /* Recommendation */
    .rec-box { background: rgba(139,92,246,0.06); border: 1px solid rgba(139,92,246,0.3); border-radius: 16px; padding: 28px; position: relative; overflow: hidden; margin-bottom: 40px; }
    .rec-box::before { content: ''; position: absolute; top: 0; left: 0; right: 0; height: 2px; background: linear-gradient(90deg, var(--primary), var(--primary-glow), var(--neon-cyan)); box-shadow: 0 0 16px rgba(139,92,246,0.6); }
    .rec-title { font-family: 'Sora', sans-serif; font-size: 16px; font-weight: 700; color: var(--primary-glow); margin-bottom: 16px; }
    .rec-body { font-size: 14px; color: var(--text-secondary); line-height: 1.7; }
    .rec-body strong { color: var(--text-primary); }

    /* Clarification note */
    .clarification { background: rgba(245,158,11,0.06); border: 1px solid rgba(245,158,11,0.2); border-radius: 12px; padding: 16px 20px; margin-bottom: 32px; font-size: 13px; color: var(--text-secondary); line-height: 1.6; }
    .clarification strong { color: var(--neon-amber); }

    .footer { text-align: center; margin-top: 56px; padding-top: 28px; border-top: 1px solid rgba(255,255,255,0.05); }
    .footer-text { font-size: 12px; color: var(--text-muted); }
    .footer-brand { font-family: 'Sora', sans-serif; font-size: 13px; font-weight: 600; background: linear-gradient(135deg, var(--primary), var(--primary-glow)); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; margin-top: 6px; }

    @media (max-width: 820px) {
      .verdict-row { grid-template-columns: 1fr; }
      .cli-grid { grid-template-columns: 1fr; }
      .spec-grid { grid-template-columns: 140px 1fr 1fr; }
    }
  </style>
</head>
<body>
<div class="container">

  <!-- Hero -->
  <div class="hero">
    <div class="hero-badge">Opta LMX ¬∑ Model Assessment ¬∑ Feb 2026</div>
    <h1>Step-3.5-Flash vs Qwen3.5<br>on Mono512</h1>
    <p class="hero-sub">Can these models run optimally on your Mac Studio M3 Ultra (512GB) with Opta LMX and CLI? Full hardware, architecture, and integration analysis.</p>
  </div>

  <!-- Clarification note -->
  <div class="clarification">
    <strong>‚ö† Model Identification Note</strong> ‚Äî "Qwen3.5 Q4_K_M: 224G" does not match any official Alibaba release exactly. The closest models are <strong>Qwen3.5-397B-A17B</strong> (Q4_K_M ‚âà 241GB) and <strong>Qwen3-235B-A22B</strong> (Q4_K_M ‚âà 124GB). The 224GB figure likely refers to <strong>Qwen3.5-397B-A17B at MXFP4_MOE (216GB) or a custom split quantization</strong>. The analysis below covers both interpretations. Step-3.5-Flash official Q4_K_S = 111GB (you cited 104G ‚Äî close, possibly a trimmed variant).
  </div>

  <!-- Verdict Cards -->
  <div class="verdict-row">

    <!-- Step-3.5-Flash -->
    <div class="verdict-card vc-green">
      <div class="vc-accent"></div>
      <div class="vc-verdict-badge badge-yes">‚úì Optimal Run</div>
      <div class="vc-model-name">Step-3.5-Flash</div>
      <div class="vc-model-sub">StepFun AI ¬∑ Q4_K_S GGUF ¬∑ ~111GB ¬∑ Released Feb 2, 2026</div>
      <div class="vc-stats">
        <div class="vc-stat">
          <div class="vc-stat-val" style="color:var(--neon-green)">111GB</div>
          <div class="vc-stat-label">On-disk size</div>
        </div>
        <div class="vc-stat">
          <div class="vc-stat-val" style="color:var(--neon-cyan)">22%</div>
          <div class="vc-stat-label">RAM usage</div>
        </div>
        <div class="vc-stat">
          <div class="vc-stat-val" style="color:var(--neon-green)">~20+</div>
          <div class="vc-stat-label">Tok/s (est.)</div>
        </div>
      </div>
      <div class="vc-reasons">
        <div class="vc-reason"><div class="vc-reason-icon">‚úÖ</div><div><strong>Comfortably fits RAM</strong> ‚Äî 111GB out of 512GB leaves 401GB headroom for KV cache, system, and second model</div></div>
        <div class="vc-reason"><div class="vc-reason-icon">‚ö°</div><div><strong>MoE architecture advantage</strong> ‚Äî only 11B active params per token. M3 Ultra's 800GB/s bandwidth does not need to load all 196B weights per step</div></div>
        <div class="vc-reason"><div class="vc-reason-icon">üî¢</div><div><strong>Multi-Token Prediction</strong> ‚Äî predicts 4 tokens simultaneously (MTP-3). Multiplies effective throughput on high-bandwidth unified memory</div></div>
        <div class="vc-reason"><div class="vc-reason-icon">üéØ</div><div><strong>Agentic by design</strong> ‚Äî 196B params across 288 experts, tuned for long-horizon tool use tasks matching Opta CLI's autonomous workloads</div></div>
        <div class="vc-reason"><div class="vc-reason-icon">‚è±</div><div><strong>Swap timeout safe</strong> ‚Äî our new size-aware timeout: <code>max(300s, 111√ó1.5)</code> = 300s. No change needed</div></div>
      </div>
    </div>

    <!-- Qwen3.5 -->
    <div class="verdict-card vc-amber">
      <div class="vc-accent"></div>
      <div class="vc-verdict-badge badge-marginal">‚ö† Marginal ‚Äî Caveats Apply</div>
      <div class="vc-model-name">Qwen3.5-397B</div>
      <div class="vc-model-sub">Alibaba Qwen ¬∑ Q4_K_M GGUF ¬∑ ~224‚Äì241GB ¬∑ Released Feb 16‚Äì24, 2026</div>
      <div class="vc-stats">
        <div class="vc-stat">
          <div class="vc-stat-val" style="color:var(--neon-amber)">241GB</div>
          <div class="vc-stat-label">Q4_K_M size</div>
        </div>
        <div class="vc-stat">
          <div class="vc-stat-val" style="color:var(--neon-amber)">47%</div>
          <div class="vc-stat-label">RAM usage</div>
        </div>
        <div class="vc-stat">
          <div class="vc-stat-val" style="color:var(--neon-amber)">~3‚Äì4</div>
          <div class="vc-stat-label">Tok/s (est.)</div>
        </div>
      </div>
      <div class="vc-reasons">
        <div class="vc-reason"><div class="vc-reason-icon">‚úÖ</div><div><strong>Technically fits</strong> ‚Äî 241GB + ~40GB KV cache + ~20GB OS = ~301GB, still within 512GB</div></div>
        <div class="vc-reason"><div class="vc-reason-icon">‚ö†Ô∏è</div><div><strong>No headroom for a second model</strong> ‚Äî cannot run alongside MiniMax or Step-3.5-Flash simultaneously</div></div>
        <div class="vc-reason"><div class="vc-reason-icon">üê¢</div><div><strong>Slow throughput</strong> ‚Äî 397B total params, even with 17B active via MoE, dense memory reads dominate at Q4_K_M. Expect 3‚Äì4 tok/s</div></div>
        <div class="vc-reason"><div class="vc-reason-icon">‚è±</div><div><strong>Swap timeout extended</strong> ‚Äî our fix: <code>max(300s, 241√ó1.5)</code> = <strong>361s (6 min)</strong>. Still might be tight if download required</div></div>
        <div class="vc-reason"><div class="vc-reason-icon">üí°</div><div><strong>Better at Q3_K_M (189GB) or Q2_K (144GB)</strong> ‚Äî dramatically more headroom and faster throughput for modest quality cost</div></div>
      </div>
    </div>
  </div>

  <div class="divider"></div>

  <!-- Architecture Spec Comparison -->
  <div class="section-label">Architecture Analysis</div>
  <h2>Side-by-Side Specs <span class="glow-line"></span></h2>

  <div class="spec-grid">
    <div class="spec-header-cell">Specification</div>
    <div class="spec-header-cell step">Step-3.5-Flash</div>
    <div class="spec-header-cell qwen">Qwen3.5-397B</div>

    <div class="spec-label">Total Params</div>
    <div class="spec-val highlight">196.81B <span class="tag-good">MoE</span></div>
    <div class="spec-val highlight">397B <span class="tag-info">MoE</span></div>

    <div class="spec-label">Active per Token</div>
    <div class="spec-val highlight" style="color:var(--neon-green)">~11B <span class="tag-good">Top-8 experts</span></div>
    <div class="spec-val highlight" style="color:var(--neon-amber)">~17B <span class="tag-warn">A17B</span></div>

    <div class="spec-label">Expert Count</div>
    <div class="spec-val">288 routed + 1 shared</div>
    <div class="spec-val">Large MoE pool</div>

    <div class="spec-label">Attention Type</div>
    <div class="spec-val">Hybrid SWA + Full (3:1)</div>
    <div class="spec-val">DeltaNet + Full (3:1)</div>

    <div class="spec-label">Token Prediction</div>
    <div class="spec-val" style="color:var(--neon-green)">MTP-3 <span class="tag-good">4√ó/forward</span></div>
    <div class="spec-val">Standard</div>

    <div class="spec-label">Context Window</div>
    <div class="spec-val">250K tokens</div>
    <div class="spec-val">256K native / 1M via YaRN</div>

    <div class="spec-label">GGUF Size (Q4)</div>
    <div class="spec-val highlight" style="color:var(--neon-green)">111GB (Q4_K_S)</div>
    <div class="spec-val highlight" style="color:var(--neon-amber)">241GB (Q4_K_M)</div>

    <div class="spec-label">Quant Options</div>
    <div class="spec-val">Q4_K_S, Q4_K_M, Q8</div>
    <div class="spec-val">Q2_K (144G), Q3_K_M (189G), Q4_K_M (241G), Q5 (282G), Q8 (422G)</div>

    <div class="spec-label">M3U RAM Used</div>
    <div class="spec-val" style="color:var(--neon-green)">111GB of 512GB (22%)</div>
    <div class="spec-val" style="color:var(--neon-amber)">241GB of 512GB (47%)</div>

    <div class="spec-label">Dual-Model Load</div>
    <div class="spec-val" style="color:var(--neon-green)">‚úÖ Easy (+MiniMax 173GB = 284GB)</div>
    <div class="spec-val" style="color:var(--neon-red)">‚ùå Risky (+any large model)</div>

    <div class="spec-label">Est. Tok/s (M3U)</div>
    <div class="spec-val" style="color:var(--neon-green)">20‚Äì35 tok/s</div>
    <div class="spec-val" style="color:var(--neon-amber)">3‚Äì6 tok/s</div>

    <div class="spec-label">Agentic Design</div>
    <div class="spec-val" style="color:var(--neon-green)">‚úÖ Explicitly designed for agents</div>
    <div class="spec-val">General-purpose, capable</div>
  </div>

  <div class="divider"></div>

  <!-- Memory Visualization -->
  <div class="section-label">Mono512 RAM Budget</div>
  <h2>Memory Layout (512GB) <span class="glow-line"></span></h2>

  <div class="memory-section">

    <!-- Step-3.5-Flash -->
    <div class="mem-card">
      <div class="mem-title">
        <span>Step-3.5-Flash Q4_K_S ‚Äî 512GB RAM</span>
        <span style="font-size:12px;color:var(--neon-green);font-family:'Sora',sans-serif;font-weight:600;">‚úì Optimal</span>
      </div>
      <div class="mem-bar-wrap">
        <!-- Model weights: 111GB = 21.7% -->
        <div style="position:absolute;left:0;top:0;bottom:0;width:21.7%;background:linear-gradient(90deg,#22c55e,#4ade80);border-radius:6px 0 0 6px;"></div>
        <!-- KV cache: 30GB = 5.9% -->
        <div style="position:absolute;left:21.7%;top:0;bottom:0;width:5.9%;background:rgba(34,197,94,0.4);"></div>
        <!-- OS/system: 20GB = 3.9% -->
        <div style="position:absolute;left:27.6%;top:0;bottom:0;width:3.9%;background:rgba(59,130,246,0.5);border-radius:0;"></div>
        <!-- MiniMax could fit: 173GB = 33.8% -->
        <div style="position:absolute;left:31.5%;top:0;bottom:0;width:33.8%;background:rgba(139,92,246,0.2);border: 1px dashed rgba(139,92,246,0.4);border-radius:0;"></div>
      </div>
      <div class="mem-labels">
        <span>0</span>
        <span style="color:var(--neon-green)">111GB model</span>
        <span style="color:var(--text-muted)">~162GB used total</span>
        <span style="color:var(--neon-green)">350GB free</span>
        <span>512GB</span>
      </div>
      <div class="mem-legend">
        <div class="mem-leg-item"><div class="mem-dot" style="background:#22c55e;"></div><span>Model weights (111GB)</span></div>
        <div class="mem-leg-item"><div class="mem-dot" style="background:rgba(34,197,94,0.4);"></div><span>KV cache est. (30GB)</span></div>
        <div class="mem-leg-item"><div class="mem-dot" style="background:rgba(59,130,246,0.5);"></div><span>OS + processes (20GB)</span></div>
        <div class="mem-leg-item"><div class="mem-dot" style="background:rgba(139,92,246,0.25);border:1px dashed rgba(139,92,246,0.5);"></div><span>Could also load MiniMax 173GB</span></div>
      </div>
    </div>

    <!-- Qwen3.5 Q4_K_M -->
    <div class="mem-card">
      <div class="mem-title">
        <span>Qwen3.5-397B Q4_K_M ‚Äî 512GB RAM</span>
        <span style="font-size:12px;color:var(--neon-amber);font-family:'Sora',sans-serif;font-weight:600;">‚ö† Marginal</span>
      </div>
      <div class="mem-bar-wrap">
        <div style="position:absolute;left:0;top:0;bottom:0;width:47.1%;background:linear-gradient(90deg,#f59e0b,#fbbf24);border-radius:6px 0 0 6px;"></div>
        <div style="position:absolute;left:47.1%;top:0;bottom:0;width:9.8%;background:rgba(245,158,11,0.4);"></div>
        <div style="position:absolute;left:56.9%;top:0;bottom:0;width:3.9%;background:rgba(59,130,246,0.5);"></div>
        <div style="position:absolute;left:60.8%;top:0;bottom:0;width:0.5%;background:rgba(239,68,68,0.5);"></div>
      </div>
      <div class="mem-labels">
        <span>0</span>
        <span style="color:var(--neon-amber)">241GB model</span>
        <span style="color:var(--neon-amber)">~311GB used total</span>
        <span style="color:var(--neon-green)">201GB free</span>
        <span>512GB</span>
      </div>
      <div class="mem-legend">
        <div class="mem-leg-item"><div class="mem-dot" style="background:#f59e0b;"></div><span>Model weights (241GB)</span></div>
        <div class="mem-leg-item"><div class="mem-dot" style="background:rgba(245,158,11,0.4);"></div><span>KV cache est. (50GB for 1M ctx)</span></div>
        <div class="mem-leg-item"><div class="mem-dot" style="background:rgba(59,130,246,0.5);"></div><span>OS + processes (20GB)</span></div>
        <div class="mem-leg-item"><div class="mem-dot" style="background:rgba(239,68,68,0.4);"></div><span>No room for second large model</span></div>
      </div>
    </div>

    <!-- Better Qwen3.5 option -->
    <div class="mem-card" style="border-color:rgba(34,197,94,0.15);">
      <div class="mem-title">
        <span>Qwen3.5-397B <strong style="color:var(--neon-green)">Q3_K_M (189GB)</strong> ‚Äî Better Alternative</span>
        <span style="font-size:12px;color:var(--neon-green);font-family:'Sora',sans-serif;font-weight:600;">‚úì Recommended</span>
      </div>
      <div class="mem-bar-wrap">
        <div style="position:absolute;left:0;top:0;bottom:0;width:36.9%;background:linear-gradient(90deg,#22c55e,#4ade80);border-radius:6px 0 0 6px;"></div>
        <div style="position:absolute;left:36.9%;top:0;bottom:0;width:7.8%;background:rgba(34,197,94,0.4);"></div>
        <div style="position:absolute;left:44.7%;top:0;bottom:0;width:3.9%;background:rgba(59,130,246,0.5);"></div>
      </div>
      <div class="mem-labels">
        <span>0</span>
        <span style="color:var(--neon-green)">189GB model</span>
        <span>~249GB used total</span>
        <span style="color:var(--neon-green)">263GB free</span>
        <span>512GB</span>
      </div>
      <div class="mem-legend">
        <div class="mem-leg-item"><div class="mem-dot" style="background:#22c55e;"></div><span>Q3_K_M weights (189GB)</span></div>
        <div class="mem-leg-item"><div class="mem-dot" style="background:rgba(34,197,94,0.4);"></div><span>KV cache est. (40GB)</span></div>
        <div class="mem-leg-item"><div class="mem-dot" style="background:rgba(59,130,246,0.5);"></div><span>OS (20GB)</span></div>
        <div style="font-size:11px;color:var(--text-muted);margin-left:auto;">Drops from 4-bit to 3-bit ¬∑ minimal quality impact on large MoE</div>
      </div>
    </div>
  </div>

  <div class="divider"></div>

  <!-- Opta CLI Integration -->
  <div class="section-label">Opta CLI Integration</div>
  <h2>How Recent Fixes Help <span class="glow-line"></span></h2>

  <div class="cli-grid">
    <div class="cli-card">
      <div class="cli-card-title">‚úÖ What Just Got Fixed (This Session)</div>
      <div class="cli-item"><div class="cli-icon">‚è±</div><div><strong>Size-aware timeout (H1)</strong> ‚Äî Step-3.5-Flash: <code>max(300s, 111√ó1.5)</code> = 300s ‚úì. Qwen3.5 Q4_K_M: <code>max(300s, 241√ó1.5)</code> = 361s ‚Äî extra 61s when it's needed most</div></div>
      <div class="cli-item"><div class="cli-icon">‚ö†Ô∏è</div><div><strong>Pre-unload download warning (H4)</strong> ‚Äî both models are &gt;10GB, so you'll see the size + timeout before the current model is unloaded. No more surprise stranding</div></div>
      <div class="cli-item"><div class="cli-icon">üîÑ</div><div><strong>Post-rollback health check (C1)</strong> ‚Äî if Qwen3.5 load fails (likely on tight RAM), rollback to previous model + LMX health verified automatically</div></div>
      <div class="cli-item"><div class="cli-icon">üõ°</div><div><strong>30s cooldown (H2)</strong> ‚Äî prevents rapid retry hammering if the 397B fails. Forces a breather before next attempt</div></div>
      <div class="cli-item"><div class="cli-icon">üíä</div><div><strong>HTTP 500 guidance (C3)</strong> ‚Äî if LMX crashes loading a huge model, you now get: "restart LMX, wait for cooldown, retry"</div></div>
    </div>

    <div class="cli-card">
      <div class="cli-card-title">üöÄ New Autonomous Session Improvements</div>
      <div class="cli-item"><div class="cli-icon">‚ù§Ô∏è</div><div><strong>LMX Watchdog (R1)</strong> ‚Äî if LMX becomes unhealthy mid-session while a large model is loaded (OOM, crash), you'll see a warning within 30s rather than silent failure</div></div>
      <div class="cli-item"><div class="cli-icon">üîÅ</div><div><strong>Headless continue (R2)</strong> ‚Äî long autonomous sessions with Step-3.5-Flash (20+ tok/s √ó many tool calls) no longer pause waiting for confirmation at <code>pauseAt</code>. Set <code>autonomy.headlessContinue: true</code> + level ‚â• 4</div></div>
      <div class="cli-item"><div class="cli-icon">üíæ</div><div><strong>Crash recovery (R3)</strong> ‚Äî if Qwen3.5's tight RAM causes an OOM kill mid-session, your last checkpoint (every 10 tool calls) survives in <code>.opta/recovery/</code></div></div>
      <div class="cli-item"><div class="cli-icon">üìã</div><div><strong>Swap command</strong> ‚Äî <code>opta models swap --from [current] --to step-3.5-flash</code>. The pre-unload warning will show 111GB and the 300s timeout upfront</div></div>
    </div>
  </div>

  <div class="divider"></div>

  <!-- Final Recommendation -->
  <div class="rec-box">
    <div class="rec-title">‚ö° Optimal Recommendation</div>
    <div class="rec-body">
      <strong>Load Step-3.5-Flash Q4_K_S (111GB) immediately ‚Äî it is the clear optimal choice.</strong> 22% RAM utilization, 20‚Äì35 tok/s throughput, 11B active params per token leveraging M3 Ultra's bandwidth efficiently, MTP-3 multiplying output, and explicitly designed for the agentic workloads Opta CLI runs. With 401GB headroom you can simultaneously run MiniMax or any second model.<br><br>
      <strong>For Qwen3.5-397B: use Q3_K_M (189GB) not Q4_K_M (241GB).</strong> The drop from 4-bit to 3-bit on a 397B MoE model is minimal in practice ‚Äî the sheer parameter count provides robustness. Q3_K_M leaves 263GB free, allows reasonable KV cache for the 256K context, and gives you 6‚Äì8 tok/s instead of 3‚Äì4 tok/s. Only load Q4_K_M if you specifically need maximum precision and are willing to sacrifice headroom and speed.<br><br>
      <strong>Never load Qwen3.5-397B alongside any other large model.</strong> At Q4_K_M + MiniMax (173GB) = 414GB just for weights ‚Äî before KV cache and OS overhead. That will OOM.
    </div>
  </div>

  <div class="footer">
    <div class="footer-text">Generated ¬∑ Feb 26 2026 ¬∑ Opta LMX Model Assessment ¬∑ Mono512 Mac Studio M3 Ultra 512GB</div>
    <div class="footer-brand">Opta ‚Äî Representation of Optimal</div>
  </div>

</div>
</body>
</html>
