---
phase: 92
plan: 02
title: LLM Service and Hybrid Routing
type: feature
wave: 2
depends_on: [92-01]
files_modified:
  - opta-native/OptaApp/OptaApp/Services/LLMService.swift
  - opta-native/OptaApp/OptaApp/Services/CloudLLMService.swift
  - opta-native/OptaApp/OptaApp/Services/LocalLLMService.swift
  - opta-native/OptaApp/OptaApp/Services/SemanticRouter.swift
  - opta-native/OptaApp/OptaApp/Services/ChatService.swift
  - opta-native/OptaApp/OptaApp/Models/ChatModels.swift
  - opta-native/OptaApp/OptaApp/Views/AiChat/AiChatView.swift
  - opta-native/OptaApp/OptaApp.xcodeproj/project.pbxproj
autonomous: true
---

<objective>
Implement the hybrid LLM routing system with local MLX inference and cloud Claude API fallback.
The SemanticRouter decides whether to use the on-device model (fast, private, simple queries)
or the cloud model (complex reasoning, optimization advice). Both services support streaming
responses that update the chat UI character-by-character.
</objective>

<execution_context>
- Local LLM: MLX Swift framework for Apple Silicon inference (mlx-swift-examples patterns)
- Cloud LLM: Anthropic Claude API via URLSession (streaming SSE)
- Routing: Query classification based on complexity, privacy needs, and network availability
- Streaming: AsyncSequence-based token streaming to ChatViewModel
- Pattern: Protocol-based services matching existing GameDetectionService/ThermalStateManager patterns
- The ChatViewModel from Plan 92-01 provides the UI state integration point
</execution_context>

<context>
@opta-native/OptaApp/OptaApp/Models/ChatModels.swift (from 92-01: ChatMessage, ChatViewModel, LLMModel)
@opta-native/OptaApp/OptaApp/Services/GameDetectionService.swift (service singleton pattern)
@opta-native/OptaApp/OptaApp/Services/ThermalStateManager.swift (service pattern)
@opta-native/OptaApp/OptaApp/Views/AiChat/AiChatView.swift (from 92-01: UI to wire to)
</context>

<tasks>

<task id="1" type="auto">
<title>Create LLM Service Protocol and Cloud Implementation</title>
<description>
Define the LLMService protocol for streaming text generation, and implement the
CloudLLMService using Anthropic's Claude API with Server-Sent Events streaming.
Include API key management via Keychain and proper error handling.
</description>
<files>
  - CREATE: opta-native/OptaApp/OptaApp/Services/LLMService.swift
  - CREATE: opta-native/OptaApp/OptaApp/Services/CloudLLMService.swift
</files>
<action>
**LLMService.swift:**
Create protocol and supporting types:

```swift
/// Protocol for LLM text generation services
protocol LLMServiceProtocol {
    /// Generate a streaming response for the given messages
    func generateStream(
        messages: [ChatMessage],
        systemPrompt: String
    ) -> AsyncThrowingStream<String, Error>

    /// Check if the service is available (model loaded / API reachable)
    var isAvailable: Bool { get async }

    /// Display name for the service
    var modelName: String { get }

    /// Cancel any in-progress generation
    func cancel()
}

/// Errors specific to LLM operations
enum LLMError: Error, LocalizedError {
    case modelNotLoaded
    case apiKeyMissing
    case networkUnavailable
    case rateLimited(retryAfter: TimeInterval)
    case generationCancelled
    case invalidResponse(String)
    case contextTooLong(maxTokens: Int)
}
```

**CloudLLMService.swift:**
Implement Claude API integration:

1. Singleton pattern: `static let shared = CloudLLMService()`
2. Properties:
   - `private var currentTask: URLSessionDataTask?`
   - `private let apiEndpoint = "https://api.anthropic.com/v1/messages"`
   - `private let model = "claude-sonnet-4-20250514"`
   - `private let maxTokens = 2048`

3. API key management:
   - Store in UserDefaults initially (with Keychain migration path noted)
   - `var apiKey: String?` computed from storage
   - `func setApiKey(_ key: String)`

4. `generateStream` implementation:
   - Build request body with messages array (convert ChatMessage to API format)
   - Set `stream: true` in request
   - Use URLSession bytes(for:) for streaming
   - Parse SSE events: `event: content_block_delta` -> extract text delta
   - Yield text tokens via AsyncThrowingStream continuation
   - Handle errors: 401 (apiKeyMissing), 429 (rateLimited), network errors

5. `cancel()`: cancel currentTask, close stream

6. Optimization context: Include system prompt about being "Opta, a system optimization AI"
   with knowledge of CPU/memory/GPU/thermal states.
</action>
<verify>
Both files compile. CloudLLMService can be instantiated.
API request format matches Anthropic Messages API specification.
Stream parsing handles SSE content_block_delta events correctly.
</verify>
<done>LLMService.swift with protocol and error types. CloudLLMService.swift with Claude API streaming implementation.</done>
</task>

<task id="2" type="auto">
<title>Create Local LLM Service and Semantic Router</title>
<description>
Implement LocalLLMService for on-device inference using MLX Swift, and SemanticRouter
that decides whether to route queries locally or to the cloud based on query complexity,
privacy requirements, and system state.
</description>
<files>
  - CREATE: opta-native/OptaApp/OptaApp/Services/LocalLLMService.swift
  - CREATE: opta-native/OptaApp/OptaApp/Services/SemanticRouter.swift
</files>
<action>
**LocalLLMService.swift:**
Implement local inference (MLX-ready with graceful fallback):

1. Singleton: `static let shared = LocalLLMService()`
2. State management:
   - `@Observable` class with `modelLoaded: Bool`, `isDownloading: Bool`, `downloadProgress: Double`
   - `modelPath: URL?` pointing to local model files
   - `private var generationTask: Task<Void, Never>?`

3. Model management:
   - `func loadModel() async throws` - loads model from disk if available
   - `func downloadModel() async throws` - downloads quantized model (placeholder URL)
   - `func isModelAvailable() -> Bool` - checks if model files exist on disk
   - Model storage: `~/Library/Application Support/OptaApp/Models/`

4. Generation (MLX-ready stub with mock fallback):
   - `generateStream` implementation that:
     a. If MLX model loaded: use MLX generation pipeline (import MLX, MLXRandom, MLXLLM patterns)
     b. If model not loaded: return informative message about downloading model
   - Streaming via AsyncThrowingStream yielding tokens
   - Temperature: 0.7, top_p: 0.9 defaults
   - Max generation: 512 tokens for local (resource conscious)

5. For initial implementation without MLX package dependency:
   - Structure code to accept MLX integration later
   - Use a simple mock that returns helpful system info responses
   - Mark MLX integration points with `// TODO: MLX Integration` comments
   - The service reports `isAvailable = false` until model is loaded

**SemanticRouter.swift:**
Decide local vs cloud routing:

1. `static let shared = SemanticRouter()`
2. `func route(query: String, context: RouterContext) -> LLMModel`:
   - Returns `.local` for: simple factual queries, status checks, privacy-sensitive
   - Returns `.cloud` for: complex reasoning, optimization advice, multi-step analysis
   - Returns based on `.auto` logic when user selects auto mode

3. `RouterContext` struct:
   - `networkAvailable: Bool`
   - `localModelLoaded: Bool`
   - `thermalState: ThermalStateViewModel`
   - `batteryLevel: Float?` (nil on desktop)
   - `userPreference: LLMModel`

4. Routing heuristics:
   - If network unavailable -> .local (forced)
   - If local model not loaded -> .cloud (forced)
   - If query is short (< 20 words) and matches simple patterns -> .local
   - If query mentions "optimize", "analyze", "compare", "explain why" -> .cloud
   - If thermal state is .serious/.critical -> .cloud (avoid local compute)
   - Default for .auto: .cloud (until local model quality proven)

5. Simple pattern matching (not ML-based, just keyword heuristics):
   - Local patterns: "what is", "how much", "status", "current", "show me"
   - Cloud patterns: "optimize", "analyze", "why is", "recommend", "compare", "help me"
</action>
<verify>
Both files compile. SemanticRouter correctly routes sample queries.
LocalLLMService gracefully handles case where model is not downloaded.
Router falls back to cloud when local unavailable.
</verify>
<done>LocalLLMService.swift with model management and generation stub. SemanticRouter.swift with keyword-based routing heuristics.</done>
</task>

<task id="3" type="auto">
<title>Create ChatService and Wire to ChatViewModel</title>
<description>
Create ChatService that orchestrates conversations using the router and LLM services,
then wire it into the ChatViewModel so the UI sends real messages and receives streaming responses.
Register all new service files in the Xcode project.
</description>
<files>
  - CREATE: opta-native/OptaApp/OptaApp/Services/ChatService.swift
  - MODIFY: opta-native/OptaApp/OptaApp/Models/ChatModels.swift
  - MODIFY: opta-native/OptaApp/OptaApp/Views/AiChat/AiChatView.swift
  - MODIFY: opta-native/OptaApp/OptaApp.xcodeproj/project.pbxproj
</files>
<action>
**ChatService.swift:**
Orchestration layer:

1. `@Observable class ChatService`:
   - `static let shared = ChatService()`
   - `private let router = SemanticRouter.shared`
   - `private let cloudService = CloudLLMService.shared`
   - `private let localService = LocalLLMService.shared`
   - `private var currentGenerationTask: Task<Void, Never>?`

2. Core method:
   ```swift
   func sendMessage(
       _ text: String,
       in viewModel: ChatViewModel,
       systemContext: String
   ) async
   ```
   - Creates user ChatMessage, appends to viewModel.messages
   - Determines route via SemanticRouter
   - Creates assistant ChatMessage with isStreaming = true
   - Calls appropriate LLMService.generateStream()
   - Updates assistant message content character by character
   - Sets isStreaming = false when complete
   - Fills in MessageMetadata (model, latency, routedLocally)

3. System prompt builder:
   ```swift
   func buildSystemPrompt(telemetry: OptaViewModel) -> String
   ```
   - Includes current CPU/memory/GPU/thermal state
   - Identifies as "Opta - System Optimization AI"
   - Provides optimization context and personality

4. Cancellation:
   - `func cancelGeneration()` - cancels task and underlying service
   - Marks current streaming message as complete with "[cancelled]"

5. Conversation templates (optimization-focused):
   - Quick optimize suggestions
   - Process analysis queries
   - Game performance questions

**ChatModels.swift modifications:**
- Update ChatViewModel.sendMessage() to call ChatService.shared
- Add `@ObservationIgnored private let chatService = ChatService.shared`
- Update sendMessage() to be async and call chatService
- Add system context from environment (telemetry data)

**AiChatView.swift modifications:**
- Pass telemetry context to ChatViewModel on appear
- Add API key setup prompt if CloudLLMService.apiKey is nil
- Show model indicator on messages (local vs cloud badge)
- Add settings gear button in header for API key configuration

**project.pbxproj:**
- Add LLMService.swift, CloudLLMService.swift, LocalLLMService.swift,
  SemanticRouter.swift, ChatService.swift to project
- All in Services group
- Unique file IDs, PBXBuildFile + PBXFileReference + PBXGroup entries

Verify build: `xcodebuild -project ... -scheme OptaApp build`
</action>
<verify>
Build passes. Sending a message in chat triggers the routing pipeline.
Cloud service attempts API call (will fail without key, but no crash).
Local service returns graceful "model not loaded" response.
Streaming text appears character by character in message bubble.
</verify>
<done>ChatService.swift created. ChatViewModel wired to services. All files in Xcode project. Build passes.</done>
</task>

</tasks>

<verification>
1. Build passes: `xcodebuild -project opta-native/OptaApp/OptaApp.xcodeproj -scheme OptaApp build`
2. Sending message routes through SemanticRouter
3. Cloud service formats correct Claude API request
4. Local service gracefully handles missing model
5. Streaming responses animate in chat bubbles
6. Cancellation stops generation cleanly
7. System context includes current telemetry data
</verification>

<success_criteria>
- [ ] LLMServiceProtocol defines streaming generation interface
- [ ] CloudLLMService implements Claude API with SSE streaming
- [ ] LocalLLMService structured for MLX with graceful fallback
- [ ] SemanticRouter classifies queries and routes appropriately
- [ ] ChatService orchestrates end-to-end conversation flow
- [ ] ChatViewModel.sendMessage() triggers full pipeline
- [ ] Streaming text updates UI in real-time
- [ ] All service files registered in Xcode project
- [ ] Build passes without errors
</success_criteria>

<output>
SUMMARY.md documenting:
- Service architecture and routing decisions
- API integration patterns
- MLX readiness status and next steps
- Streaming implementation details
- Build verification results
</output>
