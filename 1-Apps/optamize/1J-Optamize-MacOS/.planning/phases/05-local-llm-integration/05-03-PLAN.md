---
phase: 05-local-llm-integration
plan: 03
type: execute
wave: 2
depends_on: ["05-01"]
files_modified: [mcp-server/src/opta_mcp/prompts.py, mcp-server/src/opta_mcp/llm.py, src/components/QuickActions.tsx, src/pages/Dashboard.tsx]
autonomous: true
---

<objective>
Create optimization-focused prompt templates and quick action buttons.

Purpose: Provide pre-built prompts for common optimization questions, making the AI assistant more useful out-of-the-box.
Output: System prompts, quick action templates, and UI buttons for common queries.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Prior plan (Ollama setup):
@.planning/phases/05-local-llm-integration/05-01-PLAN.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create optimization-focused prompts module</name>
  <files>mcp-server/src/opta_mcp/prompts.py</files>
  <action>
Create `prompts.py` module with:

1. `SYSTEM_PROMPT` for the AI assistant:
   ```python
   SYSTEM_PROMPT = """You are Opta's AI optimization assistant, specialized in PC and gaming performance.

   Your expertise includes:
   - Windows, macOS, and Linux system optimization
   - GPU settings (NVIDIA, AMD, Intel)
   - Game-specific performance tuning
   - Background process management
   - Hardware monitoring and diagnostics

   Guidelines:
   - Be concise and actionable
   - Provide specific steps, not vague advice
   - Warn about risks for system modifications
   - Recommend safe optimizations first
   - Ask clarifying questions if needed

   You have access to the user's system information when relevant context is provided.
   """
   ```

2. `QUICK_PROMPTS` dict with pre-built queries:
   ```python
   QUICK_PROMPTS = {
       "boost_fps": {
           "label": "Boost FPS",
           "prompt": "What are the top 5 things I can do right now to boost my FPS in games?",
           "icon": "zap"
       },
       "reduce_stuttering": {
           "label": "Fix Stuttering",
           "prompt": "My games are stuttering. What should I check and how do I fix it?",
           "icon": "activity"
       },
       "startup_cleanup": {
           "label": "Faster Startup",
           "prompt": "How can I make my PC start up faster? What programs should I disable?",
           "icon": "rocket"
       },
       "gpu_optimize": {
           "label": "GPU Settings",
           "prompt": "What GPU driver settings should I adjust for better gaming performance?",
           "icon": "cpu"
       },
       "memory_management": {
           "label": "Free Up RAM",
           "prompt": "How can I free up RAM and reduce memory usage for gaming?",
           "icon": "database"
       }
   }
   ```

3. `get_system_context()` function:
   - Build context string from telemetry data
   - Include: CPU, RAM, GPU if available
   - Format for LLM consumption
  </action>
  <verify>
cd mcp-server && uv run python -c "from opta_mcp.prompts import SYSTEM_PROMPT, QUICK_PROMPTS; print(len(QUICK_PROMPTS), 'quick prompts defined')"
  </verify>
  <done>Prompts module created with system prompt and quick actions</done>
</task>

<task type="auto">
  <name>Task 2: Integrate system prompt into LLM chat</name>
  <files>mcp-server/src/opta_mcp/llm.py</files>
  <action>
1. Update `chat_completion()` to use system prompt:
   - Prepend SYSTEM_PROMPT as first message
   - Optionally include system context from telemetry

2. Add `chat_with_context()` function:
   ```python
   def chat_with_context(user_message: str, include_system_info: bool = True):
       from opta_mcp.prompts import SYSTEM_PROMPT, get_system_context
       from opta_mcp.telemetry import get_system_snapshot

       messages = [{"role": "system", "content": SYSTEM_PROMPT}]

       if include_system_info:
           snapshot = get_system_snapshot()
           context = get_system_context(snapshot)
           messages.append({"role": "system", "content": f"Current system state:\n{context}"})

       messages.append({"role": "user", "content": user_message})

       return chat_completion(messages)
   ```

3. Register `llm_chat_optimized` MCP tool:
   - Uses system prompt and context automatically
   - Simpler API for frontend

This makes the AI assistant knowledgeable about optimization by default.
  </action>
  <verify>
cd mcp-server && uv run python -c "from opta_mcp.llm import chat_with_context; print('chat_with_context available')"
  </verify>
  <done>LLM chat integrates system prompt and telemetry context</done>
</task>

<task type="auto">
  <name>Task 3: Create QuickActions component</name>
  <files>src/components/QuickActions.tsx, src/pages/Dashboard.tsx</files>
  <action>
1. Create `QuickActions.tsx`:
   - Grid of quick action buttons
   - Each button: icon + label
   - On click: send predefined prompt to chat
   - Props: onAction(prompt: string)

2. Design (Tailwind + shadcn):
   - Grid layout (2-3 columns)
   - Buttons with subtle hover glow
   - Icons for each action (use Lucide icons from shadcn setup)
   - Compact but touch-friendly

3. Quick actions to include:
   - "Boost FPS" (zap icon)
   - "Fix Stuttering" (activity icon)
   - "Faster Startup" (rocket icon)
   - "GPU Settings" (cpu icon)
   - "Free Up RAM" (database icon)

4. Integration with Dashboard:
   - Show QuickActions near ChatInterface
   - Clicking action opens chat and sends message
   - Visual feedback when action triggered

Make it easy for users to get value immediately without typing.
  </action>
  <verify>npm run dev (quick action buttons visible and trigger chat)</verify>
  <done>QuickActions component created and integrated with chat</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Prompts module imports without error
- [ ] System prompt included in LLM responses
- [ ] Quick action buttons render on Dashboard
- [ ] Clicking quick action sends message to chat
- [ ] `npm run build` succeeds
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- AI assistant responds with optimization expertise
- Quick actions make the AI immediately useful
</success_criteria>

<output>
After completion, create `.planning/phases/05-local-llm-integration/05-03-SUMMARY.md`
</output>
